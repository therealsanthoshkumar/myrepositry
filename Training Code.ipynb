{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/therealsanthoshkumar/myrepositry/blob/main/Training%20Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MobileNetV2 Training Script for Kria KV260 Deployment\n",
        "Updated with modern PyTorch quantization API (PyTorch 2.0+)\n",
        "\n",
        "Implements best practices for edge AI deployment including:\n",
        "- Post-Training Quantization (PTQ) - simpler and more compatible\n",
        "- Model optimization for DPU\n",
        "- ONNX export\n",
        "- Comprehensive evaluation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Training configuration optimized for Kria KV260\"\"\"\n",
        "\n",
        "    # Data settings\n",
        "    DATA_DIR = \"./data\"  # Update this to your dataset path\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 4\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.1\n",
        "    TEST_SPLIT = 0.1\n",
        "\n",
        "    # Model settings - Kria KV260 optimization\n",
        "    INPUT_SIZE = 224  # Standard MobileNetV2 input\n",
        "    NUM_CLASSES = 2  # Binary classification (update for your use case)\n",
        "    PRETRAINED = True\n",
        "\n",
        "    # Training settings\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    EARLY_STOPPING_PATIENCE = 7\n",
        "\n",
        "    # Quantization settings for Kria KV260 DPU\n",
        "    # Using Post-Training Quantization (PTQ) instead of QAT for better compatibility\n",
        "    QUANTIZE = True\n",
        "\n",
        "    # Export settings\n",
        "    EXPORT_ONNX = True\n",
        "    ONNX_OPSET = 11  # Compatible with most deployment tools\n",
        "\n",
        "    # Output settings\n",
        "    OUTPUT_DIR = \"./outputs\"\n",
        "    CHECKPOINT_DIR = \"./checkpoints\"\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "def get_data_transforms():\n",
        "    \"\"\"\n",
        "    Define data augmentation and normalization for MobileNetV2\n",
        "    Uses ImageNet statistics for transfer learning\n",
        "    \"\"\"\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(Config.INPUT_SIZE),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(Config.INPUT_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform\n",
        "\n",
        "def prepare_dataloaders():\n",
        "    \"\"\"Prepare train, validation, and test dataloaders\"\"\"\n",
        "\n",
        "    train_transform, val_test_transform = get_data_transforms()\n",
        "\n",
        "    # Load full dataset\n",
        "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(Config.TRAIN_SPLIT * total_size)\n",
        "    val_size = int(Config.VAL_SPLIT * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Apply transforms\n",
        "    train_dataset.dataset.transform = train_transform\n",
        "    val_dataset.dataset.transform = val_test_transform\n",
        "    test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
        "    print(f\"Class names: {full_dataset.classes}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, full_dataset.classes\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "def create_mobilenetv2_model(num_classes=Config.NUM_CLASSES, pretrained=Config.PRETRAINED):\n",
        "    \"\"\"\n",
        "    Create MobileNetV2 model optimized for Kria KV260\n",
        "\n",
        "    Key optimizations:\n",
        "    - Uses pretrained weights for better convergence\n",
        "    - Modifies classifier for custom number of classes\n",
        "    - Adds dropout for regularization\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        model = models.mobilenet_v2(weights=None)\n",
        "\n",
        "    # Modify the classifier for custom number of classes\n",
        "    # MobileNetV2 has 1280 features before the classifier\n",
        "    in_features = model.classifier[1].in_features\n",
        "\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "# ============================================================================\n",
        "# POST-TRAINING QUANTIZATION (Simpler and more compatible than QAT)\n",
        "# ============================================================================\n",
        "\n",
        "def calibrate_model(model, dataloader, device, num_batches=100):\n",
        "    \"\"\"\n",
        "    Calibrate model for quantization using sample data\n",
        "    This is used for Post-Training Quantization (PTQ)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(tqdm(dataloader, desc=\"Calibrating\")):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            model(inputs)\n",
        "\n",
        "def quantize_model_ptq(model, val_loader, device):\n",
        "    \"\"\"\n",
        "    Apply Post-Training Quantization (PTQ)\n",
        "    More reliable than QAT for deployment to Kria KV260\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß Applying Post-Training Quantization...\")\n",
        "\n",
        "    # Move model to CPU for quantization\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "\n",
        "    # Set quantization configuration\n",
        "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "    # Prepare for quantization (inserts observers)\n",
        "    model_prepared = torch.quantization.prepare(model, inplace=False)\n",
        "\n",
        "    # Calibrate with representative data\n",
        "    calibrate_model(model_prepared, val_loader, torch.device('cpu'), num_batches=50)\n",
        "\n",
        "    # Convert to quantized model\n",
        "    model_quantized = torch.quantization.convert(model_prepared, inplace=False)\n",
        "\n",
        "    print(\"‚úì Post-Training Quantization completed\")\n",
        "\n",
        "    return model_quantized\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Main training function with all best practices\"\"\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"MobileNetV2 Training for Kria KV260 Deployment\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {Config.DEVICE}\")\n",
        "    print(f\"Quantization: {'PTQ (Post-Training)' if Config.QUANTIZE else 'Disabled'}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"\\nPreparing data...\")\n",
        "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
        "\n",
        "    # Create model\n",
        "    print(\"\\nCreating model...\")\n",
        "    model = create_mobilenetv2_model(num_classes=Config.NUM_CLASSES)\n",
        "    model = model.to(Config.DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                          lr=Config.LEARNING_RATE,\n",
        "                          weight_decay=Config.WEIGHT_DECAY)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Starting Training...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{Config.NUM_EPOCHS}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, Config.DEVICE)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, Config.DEVICE)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            early_stopping_counter = 0\n",
        "            checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"‚úì Best model saved (Val Acc: {val_acc:.4f})\")\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n‚ö† Early stopping triggered after {epoch + 1} epochs\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Training Completed!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    print(\"\\nLoading best model...\")\n",
        "    checkpoint = torch.load(os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"‚úì Best model loaded (Val Acc: {checkpoint['val_acc']:.4f})\")\n",
        "\n",
        "    # Apply quantization if enabled\n",
        "    model_quantized = None\n",
        "    if Config.QUANTIZE:\n",
        "        model_quantized = quantize_model_ptq(model, val_loader, Config.DEVICE)\n",
        "\n",
        "    return model, model_quantized, history, class_names, test_loader\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, test_loader, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Evaluating {model_name} on Test Set...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Handle quantized models (output may be different type)\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            else:\n",
        "                # For quantized models, convert to tensor\n",
        "                outputs_tensor = torch.from_numpy(outputs) if isinstance(outputs, np.ndarray) else outputs\n",
        "                probs = torch.softmax(outputs_tensor, dim=1)\n",
        "                _, preds = torch.max(outputs_tensor, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nüìä Test Set Metrics:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        'model': model_name,\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1),\n",
        "        'confusion_matrix': conf_matrix.tolist(),\n",
        "        'class_names': class_names\n",
        "    }\n",
        "\n",
        "    return metrics, conf_matrix\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EXPORT (for Kria KV260)\n",
        "# ============================================================================\n",
        "\n",
        "def export_model(model, model_quantized, class_names):\n",
        "    \"\"\"Export model for Kria KV260 deployment\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Exporting Model for Deployment...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    model = model.cpu()\n",
        "\n",
        "    # 1. Save PyTorch model (FP32)\n",
        "    pytorch_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_fp32.pth')\n",
        "    torch.save(model.state_dict(), pytorch_path)\n",
        "    print(f\"‚úì FP32 PyTorch model saved to {pytorch_path}\")\n",
        "\n",
        "    # 2. Save quantized model if available\n",
        "    if model_quantized is not None:\n",
        "        quant_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_int8.pth')\n",
        "        torch.save(model_quantized.state_dict(), quant_path)\n",
        "        print(f\"‚úì INT8 Quantized model saved to {quant_path}\")\n",
        "\n",
        "    # 3. Export to ONNX (for Vitis AI deployment on Kria)\n",
        "    if Config.EXPORT_ONNX:\n",
        "        dummy_input = torch.randn(1, 3, Config.INPUT_SIZE, Config.INPUT_SIZE)\n",
        "        onnx_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_model.onnx')\n",
        "\n",
        "        try:\n",
        "            torch.onnx.export(\n",
        "                model,\n",
        "                dummy_input,\n",
        "                onnx_path,\n",
        "                export_params=True,\n",
        "                opset_version=Config.ONNX_OPSET,\n",
        "                do_constant_folding=True,\n",
        "                input_names=['input'],\n",
        "                output_names=['output'],\n",
        "                dynamic_axes={\n",
        "                    'input': {0: 'batch_size'},\n",
        "                    'output': {0: 'batch_size'}\n",
        "                }\n",
        "            )\n",
        "            print(f\"‚úì ONNX model saved to {onnx_path}\")\n",
        "\n",
        "            # Verify ONNX model\n",
        "            try:\n",
        "                import onnx\n",
        "                onnx_model = onnx.load(onnx_path)\n",
        "                onnx.checker.check_model(onnx_model)\n",
        "                print(\"‚úì ONNX model verification passed\")\n",
        "            except ImportError:\n",
        "                print(\"‚ö† ONNX not installed, skipping verification\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† ONNX verification warning: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† ONNX export failed: {e}\")\n",
        "\n",
        "    # 4. Save model metadata\n",
        "    metadata = {\n",
        "        'model_name': 'MobileNetV2',\n",
        "        'input_size': Config.INPUT_SIZE,\n",
        "        'num_classes': Config.NUM_CLASSES,\n",
        "        'class_names': class_names,\n",
        "        'quantized': Config.QUANTIZE,\n",
        "        'quantization_type': 'PTQ' if Config.QUANTIZE else 'None',\n",
        "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'preprocessing': {\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225]\n",
        "        },\n",
        "        'framework': 'PyTorch',\n",
        "        'pytorch_version': torch.__version__\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(Config.OUTPUT_DIR, 'model_metadata.json')\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "    print(f\"‚úì Model metadata saved to {metadata_path}\")\n",
        "\n",
        "    # 5. Create deployment guide\n",
        "    deployment_guide = f\"\"\"\n",
        "# MobileNetV2 Deployment Guide for Kria KV260\n",
        "\n",
        "## Model Information\n",
        "- Architecture: MobileNetV2\n",
        "- Input Size: {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Number of Classes: {Config.NUM_CLASSES}\n",
        "- Classes: {', '.join(class_names)}\n",
        "- Quantization: {'PTQ (INT8)' if Config.QUANTIZE else 'FP32 only'}\n",
        "\n",
        "## Files Generated\n",
        "1. mobilenetv2_fp32.pth - PyTorch FP32 model weights\n",
        "2. mobilenetv2_int8.pth - PyTorch INT8 quantized model (if enabled)\n",
        "3. mobilenetv2_model.onnx - ONNX format for Vitis AI\n",
        "4. model_metadata.json - Model configuration\n",
        "5. test_metrics.json - Performance metrics\n",
        "\n",
        "## Preprocessing Requirements\n",
        "Images must be preprocessed with:\n",
        "- Resize to 256x256\n",
        "- Center crop to {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Normalize with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "\n",
        "## Deployment Steps for Kria KV260\n",
        "\n",
        "### Method 1: Using Vitis AI (Recommended)\n",
        "\n",
        "#### Step 1: Quantize ONNX Model\n",
        "```bash\n",
        "# On development machine with Vitis AI Docker\n",
        "./docker_run.sh xilinx/vitis-ai-pytorch:latest\n",
        "\n",
        "# Inside container\n",
        "vai_q_pytorch \\\\\n",
        "  --model mobilenetv2_model.onnx \\\\\n",
        "  --quant_mode calib \\\\\n",
        "  --output_dir quantized/\n",
        "```\n",
        "\n",
        "#### Step 2: Compile for DPU\n",
        "```bash\n",
        "vai_c_xir \\\\\n",
        "  --xmodel quantized/mobilenetv2_int.xmodel \\\\\n",
        "  --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json \\\\\n",
        "  --output_dir compiled/\n",
        "```\n",
        "\n",
        "#### Step 3: Deploy on Kria KV260\n",
        "```python\n",
        "import vart\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load DPU runner\n",
        "dpu = vart.Runner.create_runner(\"mobilenetv2.xmodel\", \"run\")\n",
        "\n",
        "# Preprocess image\n",
        "img = Image.open(\"test.jpg\").resize(({Config.INPUT_SIZE}, {Config.INPUT_SIZE}))\n",
        "img = np.array(img).astype(np.float32) / 255.0\n",
        "img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "img = np.expand_dims(img.transpose(2, 0, 1), 0)\n",
        "\n",
        "# Run inference\n",
        "output = dpu.execute_async([img])[0]\n",
        "prediction = np.argmax(output)\n",
        "```\n",
        "\n",
        "### Method 2: Using PyTorch on Kria (No DPU)\n",
        "\n",
        "If deploying without DPU acceleration:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "model = mobilenet_v2(num_classes={Config.NUM_CLASSES})\n",
        "model.load_state_dict(torch.load('mobilenetv2_fp32.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop({Config.INPUT_SIZE}),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Inference\n",
        "img = Image.open(\"test.jpg\")\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    prediction = output.argmax(1).item()\n",
        "```\n",
        "\n",
        "## Expected Performance\n",
        "\n",
        "### FP32 Model\n",
        "- Accuracy: See test_metrics_fp32.json\n",
        "- CPU Inference: ~50-100ms per image\n",
        "- Memory: ~14MB\n",
        "\n",
        "### INT8 Quantized Model (with DPU)\n",
        "- Accuracy: See test_metrics_int8.json (typically 0.5-1% drop from FP32)\n",
        "- DPU Inference: ~15-30ms per image\n",
        "- Throughput: ~30-60 FPS\n",
        "- Memory: ~3.5MB\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### ONNX Export Issues\n",
        "- Ensure PyTorch >= 2.0\n",
        "- Try reducing opset version if needed\n",
        "- Check for custom layers\n",
        "\n",
        "### Quantization Accuracy Drop\n",
        "- Increase calibration batches\n",
        "- Use more representative calibration data\n",
        "- Consider fine-tuning after quantization\n",
        "\n",
        "### DPU Compilation Errors\n",
        "- Verify arch.json matches your Kria version\n",
        "- Check ONNX model compatibility\n",
        "- Ensure all operations are DPU-supported\n",
        "\n",
        "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "\n",
        "    guide_path = os.path.join(Config.OUTPUT_DIR, 'DEPLOYMENT_GUIDE.md')\n",
        "    with open(guide_path, 'w') as f:\n",
        "        f.write(deployment_guide)\n",
        "    print(f\"‚úì Deployment guide saved to {guide_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "    axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = os.path.join(Config.OUTPUT_DIR, 'training_history.png')\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Training history plot saved to {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
        "    cm_path = os.path.join(Config.OUTPUT_DIR, filename)\n",
        "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Confusion matrix saved to {cm_path}\")\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Train model\n",
        "        model, model_quantized, history, class_names, test_loader = train_model()\n",
        "\n",
        "        # Plot training history\n",
        "        plot_training_history(history)\n",
        "\n",
        "        # Evaluate FP32 model\n",
        "        metrics_fp32, cm_fp32 = evaluate_model(model.to(Config.DEVICE), test_loader, class_names, \"FP32 Model\")\n",
        "        plot_confusion_matrix(cm_fp32, class_names, \"FP32 Model\")\n",
        "\n",
        "        # Save FP32 metrics\n",
        "        with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_fp32.json'), 'w') as f:\n",
        "            json.dump(metrics_fp32, f, indent=4)\n",
        "\n",
        "        # Evaluate quantized model if available\n",
        "        if model_quantized is not None:\n",
        "            metrics_int8, cm_int8 = evaluate_model(model_quantized, test_loader, class_names, \"INT8 Quantized\")\n",
        "            plot_confusion_matrix(cm_int8, class_names, \"INT8 Quantized\")\n",
        "\n",
        "            # Save INT8 metrics\n",
        "            with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_int8.json'), 'w') as f:\n",
        "                json.dump(metrics_int8, f, indent=4)\n",
        "\n",
        "            # Compare metrics\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"Model Comparison\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"FP32 Accuracy:  {metrics_fp32['accuracy']:.4f}\")\n",
        "            print(f\"INT8 Accuracy:  {metrics_int8['accuracy']:.4f}\")\n",
        "            print(f\"Accuracy Drop:  {(metrics_fp32['accuracy'] - metrics_int8['accuracy']):.4f}\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        # Export models\n",
        "        export_model(model, model_quantized, class_names)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚úì Pipeline Completed Successfully!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nAll outputs saved to: {Config.OUTPUT_DIR}\")\n",
        "        print(f\"Model checkpoints saved to: {Config.CHECKPOINT_DIR}\")\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Review test_metrics_*.json for model performance\")\n",
        "        print(\"2. Check DEPLOYMENT_GUIDE.md for Kria KV260 deployment\")\n",
        "        print(\"3. Use Vitis AI to compile the ONNX model for DPU\")\n",
        "        if model_quantized:\n",
        "            print(\"4. Compare FP32 vs INT8 performance trade-offs\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"- Check if DATA_DIR path is correct\")\n",
        "        print(\"- Verify dataset has at least 2 classes\")\n",
        "        print(\"- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\")\n",
        "        print(\"- Check all required packages are installed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9daaf124-6a38-4823-9275-84270bdca410",
        "id": "My_xhbnSPF8q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MobileNetV2 Training for Kria KV260 Deployment\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Quantization: PTQ (Post-Training)\n",
            "======================================================================\n",
            "\n",
            "Preparing data...\n",
            "\n",
            "‚ùå Error occurred: [Errno 2] No such file or directory: './data'\n",
            "\n",
            "Troubleshooting tips:\n",
            "- Check if DATA_DIR path is correct\n",
            "- Verify dataset has at least 2 classes\n",
            "- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\n",
            "- Check all required packages are installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 763, in main\n",
            "    model, model_quantized, history, class_names, test_loader = train_model()\n",
            "                                                                ^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 320, in train_model\n",
            "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 106, in prepare_dataloaders\n",
            "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 328, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 149, in __init__\n",
            "    classes, class_to_idx = self.find_classes(self.root)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 234, in find_classes\n",
            "    return find_classes(directory)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 41, in find_classes\n",
            "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
            "                                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MobileNetV2 Training Script for Kria KV260 Deployment\n",
        "Updated with modern PyTorch quantization API (PyTorch 2.0+)\n",
        "\n",
        "Implements best practices for edge AI deployment including:\n",
        "- Post-Training Quantization (PTQ) - simpler and more compatible\n",
        "- Model optimization for DPU\n",
        "- ONNX export\n",
        "- Comprehensive evaluation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Training configuration optimized for Kria KV260\"\"\"\n",
        "\n",
        "    # Data settings\n",
        "    DATA_DIR = \"./data\"  # Update this to your dataset path\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 4\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.1\n",
        "    TEST_SPLIT = 0.1\n",
        "\n",
        "    # Model settings - Kria KV260 optimization\n",
        "    INPUT_SIZE = 224  # Standard MobileNetV2 input\n",
        "    NUM_CLASSES = 2  # Binary classification (update for your use case)\n",
        "    PRETRAINED = True\n",
        "\n",
        "    # Training settings\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    EARLY_STOPPING_PATIENCE = 7\n",
        "\n",
        "    # Quantization settings for Kria KV260 DPU\n",
        "    # Using Post-Training Quantization (PTQ) instead of QAT for better compatibility\n",
        "    QUANTIZE = True\n",
        "\n",
        "    # Export settings\n",
        "    EXPORT_ONNX = True\n",
        "    ONNX_OPSET = 11  # Compatible with most deployment tools\n",
        "\n",
        "    # Output settings\n",
        "    OUTPUT_DIR = \"./outputs\"\n",
        "    CHECKPOINT_DIR = \"./checkpoints\"\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "def get_data_transforms():\n",
        "    \"\"\"\n",
        "    Define data augmentation and normalization for MobileNetV2\n",
        "    Uses ImageNet statistics for transfer learning\n",
        "    \"\"\"\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(Config.INPUT_SIZE),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(Config.INPUT_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform\n",
        "\n",
        "def prepare_dataloaders():\n",
        "    \"\"\"Prepare train, validation, and test dataloaders\"\"\"\n",
        "\n",
        "    train_transform, val_test_transform = get_data_transforms()\n",
        "\n",
        "    # Load full dataset\n",
        "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(Config.TRAIN_SPLIT * total_size)\n",
        "    val_size = int(Config.VAL_SPLIT * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Apply transforms\n",
        "    train_dataset.dataset.transform = train_transform\n",
        "    val_dataset.dataset.transform = val_test_transform\n",
        "    test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
        "    print(f\"Class names: {full_dataset.classes}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, full_dataset.classes\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "def create_mobilenetv2_model(num_classes=Config.NUM_CLASSES, pretrained=Config.PRETRAINED):\n",
        "    \"\"\"\n",
        "    Create MobileNetV2 model optimized for Kria KV260\n",
        "\n",
        "    Key optimizations:\n",
        "    - Uses pretrained weights for better convergence\n",
        "    - Modifies classifier for custom number of classes\n",
        "    - Adds dropout for regularization\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        model = models.mobilenet_v2(weights=None)\n",
        "\n",
        "    # Modify the classifier for custom number of classes\n",
        "    # MobileNetV2 has 1280 features before the classifier\n",
        "    in_features = model.classifier[1].in_features\n",
        "\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "# ============================================================================\n",
        "# POST-TRAINING QUANTIZATION (Simpler and more compatible than QAT)\n",
        "# ============================================================================\n",
        "\n",
        "def calibrate_model(model, dataloader, device, num_batches=100):\n",
        "    \"\"\"\n",
        "    Calibrate model for quantization using sample data\n",
        "    This is used for Post-Training Quantization (PTQ)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(tqdm(dataloader, desc=\"Calibrating\")):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            model(inputs)\n",
        "\n",
        "def quantize_model_ptq(model, val_loader, device):\n",
        "    \"\"\"\n",
        "    Apply Post-Training Quantization (PTQ)\n",
        "    More reliable than QAT for deployment to Kria KV260\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß Applying Post-Training Quantization...\")\n",
        "\n",
        "    # Move model to CPU for quantization\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "\n",
        "    # Set quantization configuration\n",
        "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "    # Prepare for quantization (inserts observers)\n",
        "    model_prepared = torch.quantization.prepare(model, inplace=False)\n",
        "\n",
        "    # Calibrate with representative data\n",
        "    calibrate_model(model_prepared, val_loader, torch.device('cpu'), num_batches=50)\n",
        "\n",
        "    # Convert to quantized model\n",
        "    model_quantized = torch.quantization.convert(model_prepared, inplace=False)\n",
        "\n",
        "    print(\"‚úì Post-Training Quantization completed\")\n",
        "\n",
        "    return model_quantized\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Main training function with all best practices\"\"\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"MobileNetV2 Training for Kria KV260 Deployment\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {Config.DEVICE}\")\n",
        "    print(f\"Quantization: {'PTQ (Post-Training)' if Config.QUANTIZE else 'Disabled'}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"\\nPreparing data...\")\n",
        "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
        "\n",
        "    # Create model\n",
        "    print(\"\\nCreating model...\")\n",
        "    model = create_mobilenetv2_model(num_classes=Config.NUM_CLASSES)\n",
        "    model = model.to(Config.DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                          lr=Config.LEARNING_RATE,\n",
        "                          weight_decay=Config.WEIGHT_DECAY)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Starting Training...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{Config.NUM_EPOCHS}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, Config.DEVICE)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, Config.DEVICE)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            early_stopping_counter = 0\n",
        "            checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"‚úì Best model saved (Val Acc: {val_acc:.4f})\")\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n‚ö† Early stopping triggered after {epoch + 1} epochs\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Training Completed!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    print(\"\\nLoading best model...\")\n",
        "    checkpoint = torch.load(os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"‚úì Best model loaded (Val Acc: {checkpoint['val_acc']:.4f})\")\n",
        "\n",
        "    # Apply quantization if enabled\n",
        "    model_quantized = None\n",
        "    if Config.QUANTIZE:\n",
        "        model_quantized = quantize_model_ptq(model, val_loader, Config.DEVICE)\n",
        "\n",
        "    return model, model_quantized, history, class_names, test_loader\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, test_loader, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Evaluating {model_name} on Test Set...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Handle quantized models (output may be different type)\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            else:\n",
        "                # For quantized models, convert to tensor\n",
        "                outputs_tensor = torch.from_numpy(outputs) if isinstance(outputs, np.ndarray) else outputs\n",
        "                probs = torch.softmax(outputs_tensor, dim=1)\n",
        "                _, preds = torch.max(outputs_tensor, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nüìä Test Set Metrics:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        'model': model_name,\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1),\n",
        "        'confusion_matrix': conf_matrix.tolist(),\n",
        "        'class_names': class_names\n",
        "    }\n",
        "\n",
        "    return metrics, conf_matrix\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EXPORT (for Kria KV260)\n",
        "# ============================================================================\n",
        "\n",
        "def export_model(model, model_quantized, class_names):\n",
        "    \"\"\"Export model for Kria KV260 deployment\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Exporting Model for Deployment...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    model = model.cpu()\n",
        "\n",
        "    # 1. Save PyTorch model (FP32)\n",
        "    pytorch_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_fp32.pth')\n",
        "    torch.save(model.state_dict(), pytorch_path)\n",
        "    print(f\"‚úì FP32 PyTorch model saved to {pytorch_path}\")\n",
        "\n",
        "    # 2. Save quantized model if available\n",
        "    if model_quantized is not None:\n",
        "        quant_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_int8.pth')\n",
        "        torch.save(model_quantized.state_dict(), quant_path)\n",
        "        print(f\"‚úì INT8 Quantized model saved to {quant_path}\")\n",
        "\n",
        "    # 3. Export to ONNX (for Vitis AI deployment on Kria)\n",
        "    if Config.EXPORT_ONNX:\n",
        "        dummy_input = torch.randn(1, 3, Config.INPUT_SIZE, Config.INPUT_SIZE)\n",
        "        onnx_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_model.onnx')\n",
        "\n",
        "        try:\n",
        "            torch.onnx.export(\n",
        "                model,\n",
        "                dummy_input,\n",
        "                onnx_path,\n",
        "                export_params=True,\n",
        "                opset_version=Config.ONNX_OPSET,\n",
        "                do_constant_folding=True,\n",
        "                input_names=['input'],\n",
        "                output_names=['output'],\n",
        "                dynamic_axes={\n",
        "                    'input': {0: 'batch_size'},\n",
        "                    'output': {0: 'batch_size'}\n",
        "                }\n",
        "            )\n",
        "            print(f\"‚úì ONNX model saved to {onnx_path}\")\n",
        "\n",
        "            # Verify ONNX model\n",
        "            try:\n",
        "                import onnx\n",
        "                onnx_model = onnx.load(onnx_path)\n",
        "                onnx.checker.check_model(onnx_model)\n",
        "                print(\"‚úì ONNX model verification passed\")\n",
        "            except ImportError:\n",
        "                print(\"‚ö† ONNX not installed, skipping verification\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† ONNX verification warning: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† ONNX export failed: {e}\")\n",
        "\n",
        "    # 4. Save model metadata\n",
        "    metadata = {\n",
        "        'model_name': 'MobileNetV2',\n",
        "        'input_size': Config.INPUT_SIZE,\n",
        "        'num_classes': Config.NUM_CLASSES,\n",
        "        'class_names': class_names,\n",
        "        'quantized': Config.QUANTIZE,\n",
        "        'quantization_type': 'PTQ' if Config.QUANTIZE else 'None',\n",
        "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'preprocessing': {\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225]\n",
        "        },\n",
        "        'framework': 'PyTorch',\n",
        "        'pytorch_version': torch.__version__\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(Config.OUTPUT_DIR, 'model_metadata.json')\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "    print(f\"‚úì Model metadata saved to {metadata_path}\")\n",
        "\n",
        "    # 5. Create deployment guide\n",
        "    deployment_guide = f\"\"\"\n",
        "# MobileNetV2 Deployment Guide for Kria KV260\n",
        "\n",
        "## Model Information\n",
        "- Architecture: MobileNetV2\n",
        "- Input Size: {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Number of Classes: {Config.NUM_CLASSES}\n",
        "- Classes: {', '.join(class_names)}\n",
        "- Quantization: {'PTQ (INT8)' if Config.QUANTIZE else 'FP32 only'}\n",
        "\n",
        "## Files Generated\n",
        "1. mobilenetv2_fp32.pth - PyTorch FP32 model weights\n",
        "2. mobilenetv2_int8.pth - PyTorch INT8 quantized model (if enabled)\n",
        "3. mobilenetv2_model.onnx - ONNX format for Vitis AI\n",
        "4. model_metadata.json - Model configuration\n",
        "5. test_metrics.json - Performance metrics\n",
        "\n",
        "## Preprocessing Requirements\n",
        "Images must be preprocessed with:\n",
        "- Resize to 256x256\n",
        "- Center crop to {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Normalize with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "\n",
        "## Deployment Steps for Kria KV260\n",
        "\n",
        "### Method 1: Using Vitis AI (Recommended)\n",
        "\n",
        "#### Step 1: Quantize ONNX Model\n",
        "```bash\n",
        "# On development machine with Vitis AI Docker\n",
        "./docker_run.sh xilinx/vitis-ai-pytorch:latest\n",
        "\n",
        "# Inside container\n",
        "vai_q_pytorch \\\\\n",
        "  --model mobilenetv2_model.onnx \\\\\n",
        "  --quant_mode calib \\\\\n",
        "  --output_dir quantized/\n",
        "```\n",
        "\n",
        "#### Step 2: Compile for DPU\n",
        "```bash\n",
        "vai_c_xir \\\\\n",
        "  --xmodel quantized/mobilenetv2_int.xmodel \\\\\n",
        "  --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json \\\\\n",
        "  --output_dir compiled/\n",
        "```\n",
        "\n",
        "#### Step 3: Deploy on Kria KV260\n",
        "```python\n",
        "import vart\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load DPU runner\n",
        "dpu = vart.Runner.create_runner(\"mobilenetv2.xmodel\", \"run\")\n",
        "\n",
        "# Preprocess image\n",
        "img = Image.open(\"test.jpg\").resize(({Config.INPUT_SIZE}, {Config.INPUT_SIZE}))\n",
        "img = np.array(img).astype(np.float32) / 255.0\n",
        "img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "img = np.expand_dims(img.transpose(2, 0, 1), 0)\n",
        "\n",
        "# Run inference\n",
        "output = dpu.execute_async([img])[0]\n",
        "prediction = np.argmax(output)\n",
        "```\n",
        "\n",
        "### Method 2: Using PyTorch on Kria (No DPU)\n",
        "\n",
        "If deploying without DPU acceleration:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "model = mobilenet_v2(num_classes={Config.NUM_CLASSES})\n",
        "model.load_state_dict(torch.load('mobilenetv2_fp32.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop({Config.INPUT_SIZE}),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Inference\n",
        "img = Image.open(\"test.jpg\")\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    prediction = output.argmax(1).item()\n",
        "```\n",
        "\n",
        "## Expected Performance\n",
        "\n",
        "### FP32 Model\n",
        "- Accuracy: See test_metrics_fp32.json\n",
        "- CPU Inference: ~50-100ms per image\n",
        "- Memory: ~14MB\n",
        "\n",
        "### INT8 Quantized Model (with DPU)\n",
        "- Accuracy: See test_metrics_int8.json (typically 0.5-1% drop from FP32)\n",
        "- DPU Inference: ~15-30ms per image\n",
        "- Throughput: ~30-60 FPS\n",
        "- Memory: ~3.5MB\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### ONNX Export Issues\n",
        "- Ensure PyTorch >= 2.0\n",
        "- Try reducing opset version if needed\n",
        "- Check for custom layers\n",
        "\n",
        "### Quantization Accuracy Drop\n",
        "- Increase calibration batches\n",
        "- Use more representative calibration data\n",
        "- Consider fine-tuning after quantization\n",
        "\n",
        "### DPU Compilation Errors\n",
        "- Verify arch.json matches your Kria version\n",
        "- Check ONNX model compatibility\n",
        "- Ensure all operations are DPU-supported\n",
        "\n",
        "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "\n",
        "    guide_path = os.path.join(Config.OUTPUT_DIR, 'DEPLOYMENT_GUIDE.md')\n",
        "    with open(guide_path, 'w') as f:\n",
        "        f.write(deployment_guide)\n",
        "    print(f\"‚úì Deployment guide saved to {guide_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "    axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = os.path.join(Config.OUTPUT_DIR, 'training_history.png')\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Training history plot saved to {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
        "    cm_path = os.path.join(Config.OUTPUT_DIR, filename)\n",
        "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Confusion matrix saved to {cm_path}\")\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Train model\n",
        "        model, model_quantized, history, class_names, test_loader = train_model()\n",
        "\n",
        "        # Plot training history\n",
        "        plot_training_history(history)\n",
        "\n",
        "        # Evaluate FP32 model\n",
        "        metrics_fp32, cm_fp32 = evaluate_model(model.to(Config.DEVICE), test_loader, class_names, \"FP32 Model\")\n",
        "        plot_confusion_matrix(cm_fp32, class_names, \"FP32 Model\")\n",
        "\n",
        "        # Save FP32 metrics\n",
        "        with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_fp32.json'), 'w') as f:\n",
        "            json.dump(metrics_fp32, f, indent=4)\n",
        "\n",
        "        # Evaluate quantized model if available\n",
        "        if model_quantized is not None:\n",
        "            metrics_int8, cm_int8 = evaluate_model(model_quantized, test_loader, class_names, \"INT8 Quantized\")\n",
        "            plot_confusion_matrix(cm_int8, class_names, \"INT8 Quantized\")\n",
        "\n",
        "            # Save INT8 metrics\n",
        "            with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_int8.json'), 'w') as f:\n",
        "                json.dump(metrics_int8, f, indent=4)\n",
        "\n",
        "            # Compare metrics\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"Model Comparison\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"FP32 Accuracy:  {metrics_fp32['accuracy']:.4f}\")\n",
        "            print(f\"INT8 Accuracy:  {metrics_int8['accuracy']:.4f}\")\n",
        "            print(f\"Accuracy Drop:  {(metrics_fp32['accuracy'] - metrics_int8['accuracy']):.4f}\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        # Export models\n",
        "        export_model(model, model_quantized, class_names)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚úì Pipeline Completed Successfully!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nAll outputs saved to: {Config.OUTPUT_DIR}\")\n",
        "        print(f\"Model checkpoints saved to: {Config.CHECKPOINT_DIR}\")\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Review test_metrics_*.json for model performance\")\n",
        "        print(\"2. Check DEPLOYMENT_GUIDE.md for Kria KV260 deployment\")\n",
        "        print(\"3. Use Vitis AI to compile the ONNX model for DPU\")\n",
        "        if model_quantized:\n",
        "            print(\"4. Compare FP32 vs INT8 performance trade-offs\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"- Check if DATA_DIR path is correct\")\n",
        "        print(\"- Verify dataset has at least 2 classes\")\n",
        "        print(\"- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\")\n",
        "        print(\"- Check all required packages are installed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9daaf124-6a38-4823-9275-84270bdca410",
        "id": "BDJ9bXqJO2NA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MobileNetV2 Training for Kria KV260 Deployment\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Quantization: PTQ (Post-Training)\n",
            "======================================================================\n",
            "\n",
            "Preparing data...\n",
            "\n",
            "‚ùå Error occurred: [Errno 2] No such file or directory: './data'\n",
            "\n",
            "Troubleshooting tips:\n",
            "- Check if DATA_DIR path is correct\n",
            "- Verify dataset has at least 2 classes\n",
            "- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\n",
            "- Check all required packages are installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 763, in main\n",
            "    model, model_quantized, history, class_names, test_loader = train_model()\n",
            "                                                                ^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 320, in train_model\n",
            "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 106, in prepare_dataloaders\n",
            "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 328, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 149, in __init__\n",
            "    classes, class_to_idx = self.find_classes(self.root)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 234, in find_classes\n",
            "    return find_classes(directory)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 41, in find_classes\n",
            "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
            "                                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MobileNetV2 Training Script for Kria KV260 Deployment\n",
        "Updated with modern PyTorch quantization API (PyTorch 2.0+)\n",
        "\n",
        "Implements best practices for edge AI deployment including:\n",
        "- Post-Training Quantization (PTQ) - simpler and more compatible\n",
        "- Model optimization for DPU\n",
        "- ONNX export\n",
        "- Comprehensive evaluation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Training configuration optimized for Kria KV260\"\"\"\n",
        "\n",
        "    # Data settings\n",
        "    DATA_DIR = \"./data\"  # Update this to your dataset path\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 4\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.1\n",
        "    TEST_SPLIT = 0.1\n",
        "\n",
        "    # Model settings - Kria KV260 optimization\n",
        "    INPUT_SIZE = 224  # Standard MobileNetV2 input\n",
        "    NUM_CLASSES = 2  # Binary classification (update for your use case)\n",
        "    PRETRAINED = True\n",
        "\n",
        "    # Training settings\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    EARLY_STOPPING_PATIENCE = 7\n",
        "\n",
        "    # Quantization settings for Kria KV260 DPU\n",
        "    # Using Post-Training Quantization (PTQ) instead of QAT for better compatibility\n",
        "    QUANTIZE = True\n",
        "\n",
        "    # Export settings\n",
        "    EXPORT_ONNX = True\n",
        "    ONNX_OPSET = 11  # Compatible with most deployment tools\n",
        "\n",
        "    # Output settings\n",
        "    OUTPUT_DIR = \"./outputs\"\n",
        "    CHECKPOINT_DIR = \"./checkpoints\"\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "def get_data_transforms():\n",
        "    \"\"\"\n",
        "    Define data augmentation and normalization for MobileNetV2\n",
        "    Uses ImageNet statistics for transfer learning\n",
        "    \"\"\"\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(Config.INPUT_SIZE),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(Config.INPUT_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform\n",
        "\n",
        "def prepare_dataloaders():\n",
        "    \"\"\"Prepare train, validation, and test dataloaders\"\"\"\n",
        "\n",
        "    train_transform, val_test_transform = get_data_transforms()\n",
        "\n",
        "    # Load full dataset\n",
        "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(Config.TRAIN_SPLIT * total_size)\n",
        "    val_size = int(Config.VAL_SPLIT * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Apply transforms\n",
        "    train_dataset.dataset.transform = train_transform\n",
        "    val_dataset.dataset.transform = val_test_transform\n",
        "    test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
        "    print(f\"Class names: {full_dataset.classes}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, full_dataset.classes\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "def create_mobilenetv2_model(num_classes=Config.NUM_CLASSES, pretrained=Config.PRETRAINED):\n",
        "    \"\"\"\n",
        "    Create MobileNetV2 model optimized for Kria KV260\n",
        "\n",
        "    Key optimizations:\n",
        "    - Uses pretrained weights for better convergence\n",
        "    - Modifies classifier for custom number of classes\n",
        "    - Adds dropout for regularization\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        model = models.mobilenet_v2(weights=None)\n",
        "\n",
        "    # Modify the classifier for custom number of classes\n",
        "    # MobileNetV2 has 1280 features before the classifier\n",
        "    in_features = model.classifier[1].in_features\n",
        "\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "# ============================================================================\n",
        "# POST-TRAINING QUANTIZATION (Simpler and more compatible than QAT)\n",
        "# ============================================================================\n",
        "\n",
        "def calibrate_model(model, dataloader, device, num_batches=100):\n",
        "    \"\"\"\n",
        "    Calibrate model for quantization using sample data\n",
        "    This is used for Post-Training Quantization (PTQ)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(tqdm(dataloader, desc=\"Calibrating\")):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            model(inputs)\n",
        "\n",
        "def quantize_model_ptq(model, val_loader, device):\n",
        "    \"\"\"\n",
        "    Apply Post-Training Quantization (PTQ)\n",
        "    More reliable than QAT for deployment to Kria KV260\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß Applying Post-Training Quantization...\")\n",
        "\n",
        "    # Move model to CPU for quantization\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "\n",
        "    # Set quantization configuration\n",
        "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "    # Prepare for quantization (inserts observers)\n",
        "    model_prepared = torch.quantization.prepare(model, inplace=False)\n",
        "\n",
        "    # Calibrate with representative data\n",
        "    calibrate_model(model_prepared, val_loader, torch.device('cpu'), num_batches=50)\n",
        "\n",
        "    # Convert to quantized model\n",
        "    model_quantized = torch.quantization.convert(model_prepared, inplace=False)\n",
        "\n",
        "    print(\"‚úì Post-Training Quantization completed\")\n",
        "\n",
        "    return model_quantized\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Main training function with all best practices\"\"\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"MobileNetV2 Training for Kria KV260 Deployment\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {Config.DEVICE}\")\n",
        "    print(f\"Quantization: {'PTQ (Post-Training)' if Config.QUANTIZE else 'Disabled'}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"\\nPreparing data...\")\n",
        "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
        "\n",
        "    # Create model\n",
        "    print(\"\\nCreating model...\")\n",
        "    model = create_mobilenetv2_model(num_classes=Config.NUM_CLASSES)\n",
        "    model = model.to(Config.DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                          lr=Config.LEARNING_RATE,\n",
        "                          weight_decay=Config.WEIGHT_DECAY)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Starting Training...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{Config.NUM_EPOCHS}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, Config.DEVICE)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, Config.DEVICE)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            early_stopping_counter = 0\n",
        "            checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"‚úì Best model saved (Val Acc: {val_acc:.4f})\")\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n‚ö† Early stopping triggered after {epoch + 1} epochs\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Training Completed!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    print(\"\\nLoading best model...\")\n",
        "    checkpoint = torch.load(os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"‚úì Best model loaded (Val Acc: {checkpoint['val_acc']:.4f})\")\n",
        "\n",
        "    # Apply quantization if enabled\n",
        "    model_quantized = None\n",
        "    if Config.QUANTIZE:\n",
        "        model_quantized = quantize_model_ptq(model, val_loader, Config.DEVICE)\n",
        "\n",
        "    return model, model_quantized, history, class_names, test_loader\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, test_loader, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Evaluating {model_name} on Test Set...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Handle quantized models (output may be different type)\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            else:\n",
        "                # For quantized models, convert to tensor\n",
        "                outputs_tensor = torch.from_numpy(outputs) if isinstance(outputs, np.ndarray) else outputs\n",
        "                probs = torch.softmax(outputs_tensor, dim=1)\n",
        "                _, preds = torch.max(outputs_tensor, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nüìä Test Set Metrics:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        'model': model_name,\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1),\n",
        "        'confusion_matrix': conf_matrix.tolist(),\n",
        "        'class_names': class_names\n",
        "    }\n",
        "\n",
        "    return metrics, conf_matrix\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EXPORT (for Kria KV260)\n",
        "# ============================================================================\n",
        "\n",
        "def export_model(model, model_quantized, class_names):\n",
        "    \"\"\"Export model for Kria KV260 deployment\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Exporting Model for Deployment...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    model = model.cpu()\n",
        "\n",
        "    # 1. Save PyTorch model (FP32)\n",
        "    pytorch_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_fp32.pth')\n",
        "    torch.save(model.state_dict(), pytorch_path)\n",
        "    print(f\"‚úì FP32 PyTorch model saved to {pytorch_path}\")\n",
        "\n",
        "    # 2. Save quantized model if available\n",
        "    if model_quantized is not None:\n",
        "        quant_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_int8.pth')\n",
        "        torch.save(model_quantized.state_dict(), quant_path)\n",
        "        print(f\"‚úì INT8 Quantized model saved to {quant_path}\")\n",
        "\n",
        "    # 3. Export to ONNX (for Vitis AI deployment on Kria)\n",
        "    if Config.EXPORT_ONNX:\n",
        "        dummy_input = torch.randn(1, 3, Config.INPUT_SIZE, Config.INPUT_SIZE)\n",
        "        onnx_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_model.onnx')\n",
        "\n",
        "        try:\n",
        "            torch.onnx.export(\n",
        "                model,\n",
        "                dummy_input,\n",
        "                onnx_path,\n",
        "                export_params=True,\n",
        "                opset_version=Config.ONNX_OPSET,\n",
        "                do_constant_folding=True,\n",
        "                input_names=['input'],\n",
        "                output_names=['output'],\n",
        "                dynamic_axes={\n",
        "                    'input': {0: 'batch_size'},\n",
        "                    'output': {0: 'batch_size'}\n",
        "                }\n",
        "            )\n",
        "            print(f\"‚úì ONNX model saved to {onnx_path}\")\n",
        "\n",
        "            # Verify ONNX model\n",
        "            try:\n",
        "                import onnx\n",
        "                onnx_model = onnx.load(onnx_path)\n",
        "                onnx.checker.check_model(onnx_model)\n",
        "                print(\"‚úì ONNX model verification passed\")\n",
        "            except ImportError:\n",
        "                print(\"‚ö† ONNX not installed, skipping verification\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† ONNX verification warning: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† ONNX export failed: {e}\")\n",
        "\n",
        "    # 4. Save model metadata\n",
        "    metadata = {\n",
        "        'model_name': 'MobileNetV2',\n",
        "        'input_size': Config.INPUT_SIZE,\n",
        "        'num_classes': Config.NUM_CLASSES,\n",
        "        'class_names': class_names,\n",
        "        'quantized': Config.QUANTIZE,\n",
        "        'quantization_type': 'PTQ' if Config.QUANTIZE else 'None',\n",
        "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'preprocessing': {\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225]\n",
        "        },\n",
        "        'framework': 'PyTorch',\n",
        "        'pytorch_version': torch.__version__\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(Config.OUTPUT_DIR, 'model_metadata.json')\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "    print(f\"‚úì Model metadata saved to {metadata_path}\")\n",
        "\n",
        "    # 5. Create deployment guide\n",
        "    deployment_guide = f\"\"\"\n",
        "# MobileNetV2 Deployment Guide for Kria KV260\n",
        "\n",
        "## Model Information\n",
        "- Architecture: MobileNetV2\n",
        "- Input Size: {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Number of Classes: {Config.NUM_CLASSES}\n",
        "- Classes: {', '.join(class_names)}\n",
        "- Quantization: {'PTQ (INT8)' if Config.QUANTIZE else 'FP32 only'}\n",
        "\n",
        "## Files Generated\n",
        "1. mobilenetv2_fp32.pth - PyTorch FP32 model weights\n",
        "2. mobilenetv2_int8.pth - PyTorch INT8 quantized model (if enabled)\n",
        "3. mobilenetv2_model.onnx - ONNX format for Vitis AI\n",
        "4. model_metadata.json - Model configuration\n",
        "5. test_metrics.json - Performance metrics\n",
        "\n",
        "## Preprocessing Requirements\n",
        "Images must be preprocessed with:\n",
        "- Resize to 256x256\n",
        "- Center crop to {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Normalize with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "\n",
        "## Deployment Steps for Kria KV260\n",
        "\n",
        "### Method 1: Using Vitis AI (Recommended)\n",
        "\n",
        "#### Step 1: Quantize ONNX Model\n",
        "```bash\n",
        "# On development machine with Vitis AI Docker\n",
        "./docker_run.sh xilinx/vitis-ai-pytorch:latest\n",
        "\n",
        "# Inside container\n",
        "vai_q_pytorch \\\\\n",
        "  --model mobilenetv2_model.onnx \\\\\n",
        "  --quant_mode calib \\\\\n",
        "  --output_dir quantized/\n",
        "```\n",
        "\n",
        "#### Step 2: Compile for DPU\n",
        "```bash\n",
        "vai_c_xir \\\\\n",
        "  --xmodel quantized/mobilenetv2_int.xmodel \\\\\n",
        "  --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json \\\\\n",
        "  --output_dir compiled/\n",
        "```\n",
        "\n",
        "#### Step 3: Deploy on Kria KV260\n",
        "```python\n",
        "import vart\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load DPU runner\n",
        "dpu = vart.Runner.create_runner(\"mobilenetv2.xmodel\", \"run\")\n",
        "\n",
        "# Preprocess image\n",
        "img = Image.open(\"test.jpg\").resize(({Config.INPUT_SIZE}, {Config.INPUT_SIZE}))\n",
        "img = np.array(img).astype(np.float32) / 255.0\n",
        "img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "img = np.expand_dims(img.transpose(2, 0, 1), 0)\n",
        "\n",
        "# Run inference\n",
        "output = dpu.execute_async([img])[0]\n",
        "prediction = np.argmax(output)\n",
        "```\n",
        "\n",
        "### Method 2: Using PyTorch on Kria (No DPU)\n",
        "\n",
        "If deploying without DPU acceleration:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "model = mobilenet_v2(num_classes={Config.NUM_CLASSES})\n",
        "model.load_state_dict(torch.load('mobilenetv2_fp32.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop({Config.INPUT_SIZE}),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Inference\n",
        "img = Image.open(\"test.jpg\")\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    prediction = output.argmax(1).item()\n",
        "```\n",
        "\n",
        "## Expected Performance\n",
        "\n",
        "### FP32 Model\n",
        "- Accuracy: See test_metrics_fp32.json\n",
        "- CPU Inference: ~50-100ms per image\n",
        "- Memory: ~14MB\n",
        "\n",
        "### INT8 Quantized Model (with DPU)\n",
        "- Accuracy: See test_metrics_int8.json (typically 0.5-1% drop from FP32)\n",
        "- DPU Inference: ~15-30ms per image\n",
        "- Throughput: ~30-60 FPS\n",
        "- Memory: ~3.5MB\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### ONNX Export Issues\n",
        "- Ensure PyTorch >= 2.0\n",
        "- Try reducing opset version if needed\n",
        "- Check for custom layers\n",
        "\n",
        "### Quantization Accuracy Drop\n",
        "- Increase calibration batches\n",
        "- Use more representative calibration data\n",
        "- Consider fine-tuning after quantization\n",
        "\n",
        "### DPU Compilation Errors\n",
        "- Verify arch.json matches your Kria version\n",
        "- Check ONNX model compatibility\n",
        "- Ensure all operations are DPU-supported\n",
        "\n",
        "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "\n",
        "    guide_path = os.path.join(Config.OUTPUT_DIR, 'DEPLOYMENT_GUIDE.md')\n",
        "    with open(guide_path, 'w') as f:\n",
        "        f.write(deployment_guide)\n",
        "    print(f\"‚úì Deployment guide saved to {guide_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "    axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = os.path.join(Config.OUTPUT_DIR, 'training_history.png')\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Training history plot saved to {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
        "    cm_path = os.path.join(Config.OUTPUT_DIR, filename)\n",
        "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Confusion matrix saved to {cm_path}\")\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Train model\n",
        "        model, model_quantized, history, class_names, test_loader = train_model()\n",
        "\n",
        "        # Plot training history\n",
        "        plot_training_history(history)\n",
        "\n",
        "        # Evaluate FP32 model\n",
        "        metrics_fp32, cm_fp32 = evaluate_model(model.to(Config.DEVICE), test_loader, class_names, \"FP32 Model\")\n",
        "        plot_confusion_matrix(cm_fp32, class_names, \"FP32 Model\")\n",
        "\n",
        "        # Save FP32 metrics\n",
        "        with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_fp32.json'), 'w') as f:\n",
        "            json.dump(metrics_fp32, f, indent=4)\n",
        "\n",
        "        # Evaluate quantized model if available\n",
        "        if model_quantized is not None:\n",
        "            metrics_int8, cm_int8 = evaluate_model(model_quantized, test_loader, class_names, \"INT8 Quantized\")\n",
        "            plot_confusion_matrix(cm_int8, class_names, \"INT8 Quantized\")\n",
        "\n",
        "            # Save INT8 metrics\n",
        "            with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_int8.json'), 'w') as f:\n",
        "                json.dump(metrics_int8, f, indent=4)\n",
        "\n",
        "            # Compare metrics\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"Model Comparison\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"FP32 Accuracy:  {metrics_fp32['accuracy']:.4f}\")\n",
        "            print(f\"INT8 Accuracy:  {metrics_int8['accuracy']:.4f}\")\n",
        "            print(f\"Accuracy Drop:  {(metrics_fp32['accuracy'] - metrics_int8['accuracy']):.4f}\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        # Export models\n",
        "        export_model(model, model_quantized, class_names)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚úì Pipeline Completed Successfully!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nAll outputs saved to: {Config.OUTPUT_DIR}\")\n",
        "        print(f\"Model checkpoints saved to: {Config.CHECKPOINT_DIR}\")\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Review test_metrics_*.json for model performance\")\n",
        "        print(\"2. Check DEPLOYMENT_GUIDE.md for Kria KV260 deployment\")\n",
        "        print(\"3. Use Vitis AI to compile the ONNX model for DPU\")\n",
        "        if model_quantized:\n",
        "            print(\"4. Compare FP32 vs INT8 performance trade-offs\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"- Check if DATA_DIR path is correct\")\n",
        "        print(\"- Verify dataset has at least 2 classes\")\n",
        "        print(\"- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\")\n",
        "        print(\"- Check all required packages are installed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9daaf124-6a38-4823-9275-84270bdca410",
        "id": "AK4zcyanO-rA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MobileNetV2 Training for Kria KV260 Deployment\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Quantization: PTQ (Post-Training)\n",
            "======================================================================\n",
            "\n",
            "Preparing data...\n",
            "\n",
            "‚ùå Error occurred: [Errno 2] No such file or directory: './data'\n",
            "\n",
            "Troubleshooting tips:\n",
            "- Check if DATA_DIR path is correct\n",
            "- Verify dataset has at least 2 classes\n",
            "- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\n",
            "- Check all required packages are installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 763, in main\n",
            "    model, model_quantized, history, class_names, test_loader = train_model()\n",
            "                                                                ^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 320, in train_model\n",
            "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 106, in prepare_dataloaders\n",
            "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 328, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 149, in __init__\n",
            "    classes, class_to_idx = self.find_classes(self.root)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 234, in find_classes\n",
            "    return find_classes(directory)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 41, in find_classes\n",
            "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
            "                                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MobileNetV2 Training Script for Kria KV260 Deployment\n",
        "Updated with modern PyTorch quantization API (PyTorch 2.0+)\n",
        "\n",
        "Implements best practices for edge AI deployment including:\n",
        "- Post-Training Quantization (PTQ) - simpler and more compatible\n",
        "- Model optimization for DPU\n",
        "- ONNX export\n",
        "- Comprehensive evaluation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Training configuration optimized for Kria KV260\"\"\"\n",
        "\n",
        "    # Data settings\n",
        "    DATA_DIR = \"./data\"  # Update this to your dataset path\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 4\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.1\n",
        "    TEST_SPLIT = 0.1\n",
        "\n",
        "    # Model settings - Kria KV260 optimization\n",
        "    INPUT_SIZE = 224  # Standard MobileNetV2 input\n",
        "    NUM_CLASSES = 2  # Binary classification (update for your use case)\n",
        "    PRETRAINED = True\n",
        "\n",
        "    # Training settings\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    EARLY_STOPPING_PATIENCE = 7\n",
        "\n",
        "    # Quantization settings for Kria KV260 DPU\n",
        "    # Using Post-Training Quantization (PTQ) instead of QAT for better compatibility\n",
        "    QUANTIZE = True\n",
        "\n",
        "    # Export settings\n",
        "    EXPORT_ONNX = True\n",
        "    ONNX_OPSET = 11  # Compatible with most deployment tools\n",
        "\n",
        "    # Output settings\n",
        "    OUTPUT_DIR = \"./outputs\"\n",
        "    CHECKPOINT_DIR = \"./checkpoints\"\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "def get_data_transforms():\n",
        "    \"\"\"\n",
        "    Define data augmentation and normalization for MobileNetV2\n",
        "    Uses ImageNet statistics for transfer learning\n",
        "    \"\"\"\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(Config.INPUT_SIZE),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(Config.INPUT_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform\n",
        "\n",
        "def prepare_dataloaders():\n",
        "    \"\"\"Prepare train, validation, and test dataloaders\"\"\"\n",
        "\n",
        "    train_transform, val_test_transform = get_data_transforms()\n",
        "\n",
        "    # Load full dataset\n",
        "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(Config.TRAIN_SPLIT * total_size)\n",
        "    val_size = int(Config.VAL_SPLIT * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Apply transforms\n",
        "    train_dataset.dataset.transform = train_transform\n",
        "    val_dataset.dataset.transform = val_test_transform\n",
        "    test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
        "    print(f\"Class names: {full_dataset.classes}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, full_dataset.classes\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "def create_mobilenetv2_model(num_classes=Config.NUM_CLASSES, pretrained=Config.PRETRAINED):\n",
        "    \"\"\"\n",
        "    Create MobileNetV2 model optimized for Kria KV260\n",
        "\n",
        "    Key optimizations:\n",
        "    - Uses pretrained weights for better convergence\n",
        "    - Modifies classifier for custom number of classes\n",
        "    - Adds dropout for regularization\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        model = models.mobilenet_v2(weights=None)\n",
        "\n",
        "    # Modify the classifier for custom number of classes\n",
        "    # MobileNetV2 has 1280 features before the classifier\n",
        "    in_features = model.classifier[1].in_features\n",
        "\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "# ============================================================================\n",
        "# POST-TRAINING QUANTIZATION (Simpler and more compatible than QAT)\n",
        "# ============================================================================\n",
        "\n",
        "def calibrate_model(model, dataloader, device, num_batches=100):\n",
        "    \"\"\"\n",
        "    Calibrate model for quantization using sample data\n",
        "    This is used for Post-Training Quantization (PTQ)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(tqdm(dataloader, desc=\"Calibrating\")):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            model(inputs)\n",
        "\n",
        "def quantize_model_ptq(model, val_loader, device):\n",
        "    \"\"\"\n",
        "    Apply Post-Training Quantization (PTQ)\n",
        "    More reliable than QAT for deployment to Kria KV260\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß Applying Post-Training Quantization...\")\n",
        "\n",
        "    # Move model to CPU for quantization\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "\n",
        "    # Set quantization configuration\n",
        "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "    # Prepare for quantization (inserts observers)\n",
        "    model_prepared = torch.quantization.prepare(model, inplace=False)\n",
        "\n",
        "    # Calibrate with representative data\n",
        "    calibrate_model(model_prepared, val_loader, torch.device('cpu'), num_batches=50)\n",
        "\n",
        "    # Convert to quantized model\n",
        "    model_quantized = torch.quantization.convert(model_prepared, inplace=False)\n",
        "\n",
        "    print(\"‚úì Post-Training Quantization completed\")\n",
        "\n",
        "    return model_quantized\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Main training function with all best practices\"\"\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"MobileNetV2 Training for Kria KV260 Deployment\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {Config.DEVICE}\")\n",
        "    print(f\"Quantization: {'PTQ (Post-Training)' if Config.QUANTIZE else 'Disabled'}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"\\nPreparing data...\")\n",
        "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
        "\n",
        "    # Create model\n",
        "    print(\"\\nCreating model...\")\n",
        "    model = create_mobilenetv2_model(num_classes=Config.NUM_CLASSES)\n",
        "    model = model.to(Config.DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                          lr=Config.LEARNING_RATE,\n",
        "                          weight_decay=Config.WEIGHT_DECAY)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Starting Training...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{Config.NUM_EPOCHS}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, Config.DEVICE)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, Config.DEVICE)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            early_stopping_counter = 0\n",
        "            checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"‚úì Best model saved (Val Acc: {val_acc:.4f})\")\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n‚ö† Early stopping triggered after {epoch + 1} epochs\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Training Completed!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    print(\"\\nLoading best model...\")\n",
        "    checkpoint = torch.load(os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"‚úì Best model loaded (Val Acc: {checkpoint['val_acc']:.4f})\")\n",
        "\n",
        "    # Apply quantization if enabled\n",
        "    model_quantized = None\n",
        "    if Config.QUANTIZE:\n",
        "        model_quantized = quantize_model_ptq(model, val_loader, Config.DEVICE)\n",
        "\n",
        "    return model, model_quantized, history, class_names, test_loader\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, test_loader, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Evaluating {model_name} on Test Set...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Handle quantized models (output may be different type)\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            else:\n",
        "                # For quantized models, convert to tensor\n",
        "                outputs_tensor = torch.from_numpy(outputs) if isinstance(outputs, np.ndarray) else outputs\n",
        "                probs = torch.softmax(outputs_tensor, dim=1)\n",
        "                _, preds = torch.max(outputs_tensor, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nüìä Test Set Metrics:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        'model': model_name,\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1),\n",
        "        'confusion_matrix': conf_matrix.tolist(),\n",
        "        'class_names': class_names\n",
        "    }\n",
        "\n",
        "    return metrics, conf_matrix\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EXPORT (for Kria KV260)\n",
        "# ============================================================================\n",
        "\n",
        "def export_model(model, model_quantized, class_names):\n",
        "    \"\"\"Export model for Kria KV260 deployment\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Exporting Model for Deployment...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    model = model.cpu()\n",
        "\n",
        "    # 1. Save PyTorch model (FP32)\n",
        "    pytorch_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_fp32.pth')\n",
        "    torch.save(model.state_dict(), pytorch_path)\n",
        "    print(f\"‚úì FP32 PyTorch model saved to {pytorch_path}\")\n",
        "\n",
        "    # 2. Save quantized model if available\n",
        "    if model_quantized is not None:\n",
        "        quant_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_int8.pth')\n",
        "        torch.save(model_quantized.state_dict(), quant_path)\n",
        "        print(f\"‚úì INT8 Quantized model saved to {quant_path}\")\n",
        "\n",
        "    # 3. Export to ONNX (for Vitis AI deployment on Kria)\n",
        "    if Config.EXPORT_ONNX:\n",
        "        dummy_input = torch.randn(1, 3, Config.INPUT_SIZE, Config.INPUT_SIZE)\n",
        "        onnx_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_model.onnx')\n",
        "\n",
        "        try:\n",
        "            torch.onnx.export(\n",
        "                model,\n",
        "                dummy_input,\n",
        "                onnx_path,\n",
        "                export_params=True,\n",
        "                opset_version=Config.ONNX_OPSET,\n",
        "                do_constant_folding=True,\n",
        "                input_names=['input'],\n",
        "                output_names=['output'],\n",
        "                dynamic_axes={\n",
        "                    'input': {0: 'batch_size'},\n",
        "                    'output': {0: 'batch_size'}\n",
        "                }\n",
        "            )\n",
        "            print(f\"‚úì ONNX model saved to {onnx_path}\")\n",
        "\n",
        "            # Verify ONNX model\n",
        "            try:\n",
        "                import onnx\n",
        "                onnx_model = onnx.load(onnx_path)\n",
        "                onnx.checker.check_model(onnx_model)\n",
        "                print(\"‚úì ONNX model verification passed\")\n",
        "            except ImportError:\n",
        "                print(\"‚ö† ONNX not installed, skipping verification\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† ONNX verification warning: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† ONNX export failed: {e}\")\n",
        "\n",
        "    # 4. Save model metadata\n",
        "    metadata = {\n",
        "        'model_name': 'MobileNetV2',\n",
        "        'input_size': Config.INPUT_SIZE,\n",
        "        'num_classes': Config.NUM_CLASSES,\n",
        "        'class_names': class_names,\n",
        "        'quantized': Config.QUANTIZE,\n",
        "        'quantization_type': 'PTQ' if Config.QUANTIZE else 'None',\n",
        "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'preprocessing': {\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225]\n",
        "        },\n",
        "        'framework': 'PyTorch',\n",
        "        'pytorch_version': torch.__version__\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(Config.OUTPUT_DIR, 'model_metadata.json')\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "    print(f\"‚úì Model metadata saved to {metadata_path}\")\n",
        "\n",
        "    # 5. Create deployment guide\n",
        "    deployment_guide = f\"\"\"\n",
        "# MobileNetV2 Deployment Guide for Kria KV260\n",
        "\n",
        "## Model Information\n",
        "- Architecture: MobileNetV2\n",
        "- Input Size: {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Number of Classes: {Config.NUM_CLASSES}\n",
        "- Classes: {', '.join(class_names)}\n",
        "- Quantization: {'PTQ (INT8)' if Config.QUANTIZE else 'FP32 only'}\n",
        "\n",
        "## Files Generated\n",
        "1. mobilenetv2_fp32.pth - PyTorch FP32 model weights\n",
        "2. mobilenetv2_int8.pth - PyTorch INT8 quantized model (if enabled)\n",
        "3. mobilenetv2_model.onnx - ONNX format for Vitis AI\n",
        "4. model_metadata.json - Model configuration\n",
        "5. test_metrics.json - Performance metrics\n",
        "\n",
        "## Preprocessing Requirements\n",
        "Images must be preprocessed with:\n",
        "- Resize to 256x256\n",
        "- Center crop to {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Normalize with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "\n",
        "## Deployment Steps for Kria KV260\n",
        "\n",
        "### Method 1: Using Vitis AI (Recommended)\n",
        "\n",
        "#### Step 1: Quantize ONNX Model\n",
        "```bash\n",
        "# On development machine with Vitis AI Docker\n",
        "./docker_run.sh xilinx/vitis-ai-pytorch:latest\n",
        "\n",
        "# Inside container\n",
        "vai_q_pytorch \\\\\n",
        "  --model mobilenetv2_model.onnx \\\\\n",
        "  --quant_mode calib \\\\\n",
        "  --output_dir quantized/\n",
        "```\n",
        "\n",
        "#### Step 2: Compile for DPU\n",
        "```bash\n",
        "vai_c_xir \\\\\n",
        "  --xmodel quantized/mobilenetv2_int.xmodel \\\\\n",
        "  --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json \\\\\n",
        "  --output_dir compiled/\n",
        "```\n",
        "\n",
        "#### Step 3: Deploy on Kria KV260\n",
        "```python\n",
        "import vart\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load DPU runner\n",
        "dpu = vart.Runner.create_runner(\"mobilenetv2.xmodel\", \"run\")\n",
        "\n",
        "# Preprocess image\n",
        "img = Image.open(\"test.jpg\").resize(({Config.INPUT_SIZE}, {Config.INPUT_SIZE}))\n",
        "img = np.array(img).astype(np.float32) / 255.0\n",
        "img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "img = np.expand_dims(img.transpose(2, 0, 1), 0)\n",
        "\n",
        "# Run inference\n",
        "output = dpu.execute_async([img])[0]\n",
        "prediction = np.argmax(output)\n",
        "```\n",
        "\n",
        "### Method 2: Using PyTorch on Kria (No DPU)\n",
        "\n",
        "If deploying without DPU acceleration:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "model = mobilenet_v2(num_classes={Config.NUM_CLASSES})\n",
        "model.load_state_dict(torch.load('mobilenetv2_fp32.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop({Config.INPUT_SIZE}),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Inference\n",
        "img = Image.open(\"test.jpg\")\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    prediction = output.argmax(1).item()\n",
        "```\n",
        "\n",
        "## Expected Performance\n",
        "\n",
        "### FP32 Model\n",
        "- Accuracy: See test_metrics_fp32.json\n",
        "- CPU Inference: ~50-100ms per image\n",
        "- Memory: ~14MB\n",
        "\n",
        "### INT8 Quantized Model (with DPU)\n",
        "- Accuracy: See test_metrics_int8.json (typically 0.5-1% drop from FP32)\n",
        "- DPU Inference: ~15-30ms per image\n",
        "- Throughput: ~30-60 FPS\n",
        "- Memory: ~3.5MB\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### ONNX Export Issues\n",
        "- Ensure PyTorch >= 2.0\n",
        "- Try reducing opset version if needed\n",
        "- Check for custom layers\n",
        "\n",
        "### Quantization Accuracy Drop\n",
        "- Increase calibration batches\n",
        "- Use more representative calibration data\n",
        "- Consider fine-tuning after quantization\n",
        "\n",
        "### DPU Compilation Errors\n",
        "- Verify arch.json matches your Kria version\n",
        "- Check ONNX model compatibility\n",
        "- Ensure all operations are DPU-supported\n",
        "\n",
        "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "\n",
        "    guide_path = os.path.join(Config.OUTPUT_DIR, 'DEPLOYMENT_GUIDE.md')\n",
        "    with open(guide_path, 'w') as f:\n",
        "        f.write(deployment_guide)\n",
        "    print(f\"‚úì Deployment guide saved to {guide_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "    axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = os.path.join(Config.OUTPUT_DIR, 'training_history.png')\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Training history plot saved to {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
        "    cm_path = os.path.join(Config.OUTPUT_DIR, filename)\n",
        "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Confusion matrix saved to {cm_path}\")\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Train model\n",
        "        model, model_quantized, history, class_names, test_loader = train_model()\n",
        "\n",
        "        # Plot training history\n",
        "        plot_training_history(history)\n",
        "\n",
        "        # Evaluate FP32 model\n",
        "        metrics_fp32, cm_fp32 = evaluate_model(model.to(Config.DEVICE), test_loader, class_names, \"FP32 Model\")\n",
        "        plot_confusion_matrix(cm_fp32, class_names, \"FP32 Model\")\n",
        "\n",
        "        # Save FP32 metrics\n",
        "        with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_fp32.json'), 'w') as f:\n",
        "            json.dump(metrics_fp32, f, indent=4)\n",
        "\n",
        "        # Evaluate quantized model if available\n",
        "        if model_quantized is not None:\n",
        "            metrics_int8, cm_int8 = evaluate_model(model_quantized, test_loader, class_names, \"INT8 Quantized\")\n",
        "            plot_confusion_matrix(cm_int8, class_names, \"INT8 Quantized\")\n",
        "\n",
        "            # Save INT8 metrics\n",
        "            with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_int8.json'), 'w') as f:\n",
        "                json.dump(metrics_int8, f, indent=4)\n",
        "\n",
        "            # Compare metrics\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"Model Comparison\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"FP32 Accuracy:  {metrics_fp32['accuracy']:.4f}\")\n",
        "            print(f\"INT8 Accuracy:  {metrics_int8['accuracy']:.4f}\")\n",
        "            print(f\"Accuracy Drop:  {(metrics_fp32['accuracy'] - metrics_int8['accuracy']):.4f}\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        # Export models\n",
        "        export_model(model, model_quantized, class_names)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚úì Pipeline Completed Successfully!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nAll outputs saved to: {Config.OUTPUT_DIR}\")\n",
        "        print(f\"Model checkpoints saved to: {Config.CHECKPOINT_DIR}\")\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Review test_metrics_*.json for model performance\")\n",
        "        print(\"2. Check DEPLOYMENT_GUIDE.md for Kria KV260 deployment\")\n",
        "        print(\"3. Use Vitis AI to compile the ONNX model for DPU\")\n",
        "        if model_quantized:\n",
        "            print(\"4. Compare FP32 vs INT8 performance trade-offs\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"- Check if DATA_DIR path is correct\")\n",
        "        print(\"- Verify dataset has at least 2 classes\")\n",
        "        print(\"- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\")\n",
        "        print(\"- Check all required packages are installed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9daaf124-6a38-4823-9275-84270bdca410",
        "id": "UGG9XyJ3O_kM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MobileNetV2 Training for Kria KV260 Deployment\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Quantization: PTQ (Post-Training)\n",
            "======================================================================\n",
            "\n",
            "Preparing data...\n",
            "\n",
            "‚ùå Error occurred: [Errno 2] No such file or directory: './data'\n",
            "\n",
            "Troubleshooting tips:\n",
            "- Check if DATA_DIR path is correct\n",
            "- Verify dataset has at least 2 classes\n",
            "- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\n",
            "- Check all required packages are installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 763, in main\n",
            "    model, model_quantized, history, class_names, test_loader = train_model()\n",
            "                                                                ^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 320, in train_model\n",
            "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 106, in prepare_dataloaders\n",
            "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 328, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 149, in __init__\n",
            "    classes, class_to_idx = self.find_classes(self.root)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 234, in find_classes\n",
            "    return find_classes(directory)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 41, in find_classes\n",
            "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
            "                                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZkrpyfpy-Yz",
        "outputId": "763b30e0-b87b-47da-c478-7c72d889a62e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.20.1)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.24.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.6)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.12.19)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (26.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision scikit-learn onnx onnxruntime tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W864WUd5zd1_",
        "outputId": "c5d0fc4b-e82f-4a86-bbcd-7c3e94d4ddc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/WasteDataset\"  # Change accordingly\n"
      ],
      "metadata": {
        "id": "qN_KZgAXLIg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Config\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 6\n",
        "EPOCHS = 50\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRZkjTAPK_Ti",
        "outputId": "6c7f3476-7641-457f-b8fe-b507479715d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Paths (already split dataset)\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/WasteDataset/train\"\n",
        "VAL_DIR   = \"/content/drive/MyDrive/WasteDataset/val\"\n",
        "\n",
        "# Transforms (DPU-friendly fixed size)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets directly (NO SPLITTING)\n",
        "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=transform_train)\n",
        "val_dataset   = datasets.ImageFolder(VAL_DIR, transform=transform_val)\n",
        "\n",
        "# DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Class names (same order for both)\n",
        "class_names = train_dataset.classes\n",
        "NUM_CLASSES = len(class_names)\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Validation size:\", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOZhlwFKLEux",
        "outputId": "083aa1b1-75c8-42c6-d4c9-b49f8d361ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
            "Train size: 2019\n",
            "Validation size: 508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Replace classifier (Output must be 2D: batch √ó classes)\n",
        "model.classifier[1] = nn.Linear(model.last_channel, NUM_CLASSES)\n",
        "\n",
        "model = model.to(DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzzMoIK_LT9y",
        "outputId": "2971ffee-a72e-4690-89a7-86d72c5652ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # ---------------- TRAIN ----------------\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # ---------------- VALIDATION ----------------\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} \"\n",
        "          f\"Val Loss: {val_loss:.4f} \"\n",
        "          f\"Val Acc: {acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if acc > best_val_acc:\n",
        "        best_val_acc = acc\n",
        "        torch.save({\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"class_names\": class_names,\n",
        "            \"input_size\": 224\n",
        "        }, \"best_model.pth\")\n",
        "\n",
        "print(\"Best Validation Accuracy:\", best_val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J0eaGIbLXV5",
        "outputId": "de0ce120-c11a-4a84-b8eb-1ec2d1cf8fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:31<00:00,  2.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Train Loss: 54.1823 Val Loss: 7.0694 Val Acc: 0.8681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/50] Train Loss: 22.1475 Val Loss: 4.6836 Val Acc: 0.9094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/50] Train Loss: 14.1194 Val Loss: 4.0437 Val Acc: 0.9232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:22<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/50] Train Loss: 8.7644 Val Loss: 3.3303 Val Acc: 0.9213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50] Train Loss: 7.7447 Val Loss: 2.9828 Val Acc: 0.9331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/50] Train Loss: 5.8879 Val Loss: 3.1996 Val Acc: 0.9311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/50] Train Loss: 4.6469 Val Loss: 3.0786 Val Acc: 0.9370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/50] Train Loss: 4.7066 Val Loss: 2.6431 Val Acc: 0.9488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/50] Train Loss: 3.2122 Val Loss: 3.0300 Val Acc: 0.9331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50] Train Loss: 2.3417 Val Loss: 2.6846 Val Acc: 0.9390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/50] Train Loss: 2.7941 Val Loss: 2.8468 Val Acc: 0.9449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/50] Train Loss: 3.4671 Val Loss: 2.2588 Val Acc: 0.9469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/50] Train Loss: 5.0903 Val Loss: 3.6871 Val Acc: 0.9173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:20<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/50] Train Loss: 5.6095 Val Loss: 3.7526 Val Acc: 0.9154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:20<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/50] Train Loss: 5.1340 Val Loss: 3.2753 Val Acc: 0.9311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/50] Train Loss: 6.4341 Val Loss: 3.5191 Val Acc: 0.9291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/50] Train Loss: 4.5015 Val Loss: 2.8601 Val Acc: 0.9488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/50] Train Loss: 4.1257 Val Loss: 3.4773 Val Acc: 0.9429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/50] Train Loss: 2.9826 Val Loss: 3.7324 Val Acc: 0.9311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/50] Train Loss: 4.2309 Val Loss: 3.2413 Val Acc: 0.9429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/50] Train Loss: 3.9875 Val Loss: 3.3788 Val Acc: 0.9331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/50] Train Loss: 1.4625 Val Loss: 2.8027 Val Acc: 0.9528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:20<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/50] Train Loss: 1.1854 Val Loss: 2.6666 Val Acc: 0.9567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/50] Train Loss: 0.9930 Val Loss: 2.5187 Val Acc: 0.9469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/50] Train Loss: 3.6355 Val Loss: 2.9839 Val Acc: 0.9528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/50] Train Loss: 2.5147 Val Loss: 2.7614 Val Acc: 0.9488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/50] Train Loss: 1.8798 Val Loss: 2.8346 Val Acc: 0.9528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/50] Train Loss: 2.8950 Val Loss: 3.0524 Val Acc: 0.9449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:20<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/50] Train Loss: 1.0361 Val Loss: 2.9945 Val Acc: 0.9390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:20<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/50] Train Loss: 0.7420 Val Loss: 3.0089 Val Acc: 0.9449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/50] Train Loss: 0.6849 Val Loss: 3.3501 Val Acc: 0.9409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/50] Train Loss: 0.6348 Val Loss: 2.7028 Val Acc: 0.9547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/50] Train Loss: 3.5457 Val Loss: 2.8443 Val Acc: 0.9508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/50] Train Loss: 4.5712 Val Loss: 3.3504 Val Acc: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/50] Train Loss: 5.7502 Val Loss: 3.7906 Val Acc: 0.9213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/50] Train Loss: 4.8735 Val Loss: 3.7438 Val Acc: 0.9232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/50] Train Loss: 1.8685 Val Loss: 3.3871 Val Acc: 0.9331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/50] Train Loss: 1.1897 Val Loss: 2.9939 Val Acc: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/50] Train Loss: 2.8743 Val Loss: 2.9314 Val Acc: 0.9429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/50] Train Loss: 1.8809 Val Loss: 3.6378 Val Acc: 0.9390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/50] Train Loss: 1.3452 Val Loss: 3.0241 Val Acc: 0.9508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/50] Train Loss: 5.9327 Val Loss: 4.1992 Val Acc: 0.9232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/50] Train Loss: 3.5775 Val Loss: 3.8456 Val Acc: 0.9252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/50] Train Loss: 3.2710 Val Loss: 3.4890 Val Acc: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/50] Train Loss: 2.5376 Val Loss: 3.9043 Val Acc: 0.9331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:20<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/50] Train Loss: 1.0731 Val Loss: 2.9658 Val Acc: 0.9449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/50] Train Loss: 0.8357 Val Loss: 2.9724 Val Acc: 0.9528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/50] Train Loss: 1.4115 Val Loss: 3.0274 Val Acc: 0.9488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/50] Train Loss: 3.0256 Val Loss: 4.3715 Val Acc: 0.9252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:21<00:00,  2.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/50] Train Loss: 1.5869 Val Loss: 4.0464 Val Acc: 0.9311\n",
            "Best Validation Accuracy: 0.9566929133858267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(\"Final Metrics:\")\n",
        "print(\"Accuracy:\", best_val_acc)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxWeYd5_TSPj",
        "outputId": "ec75907b-940f-4faa-f601-03e64a5d2a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Metrics:\n",
            "Accuracy: 0.9566929133858267\n",
            "Precision: 0.9359118928020793\n",
            "Recall: 0.9311023622047244\n",
            "F1 Score: 0.9307775728867141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def predict_image(image_path):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform_val(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "    print(\"Predicted Class:\", class_names[pred.item()])\n",
        "\n",
        "# Example:\n",
        "predict_image(\"/content/drive/MyDrive/TrashNet/glass/glass100.jpg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DljN7MJTTVX4",
        "outputId": "559f1201-6ac2-4fa0-b0a0-c286fa93b366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: glass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "61b4toK2UfDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_DIR = \"/content/drive/MyDrive/WasteDataset/val\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],\n",
        "                         [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_dataset = datasets.ImageFolder(VAL_DIR, transform=transform)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class_names = val_dataset.classes\n",
        "NUM_CLASSES = len(class_names)\n",
        "\n",
        "print(\"Classes:\", class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "359RbMO0Vkk-",
        "outputId": "b78a25eb-9515-4291-b537-38c885ef7006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(DEVICE)\n",
        "\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "all_preds  = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "all_probs  = np.array(all_probs)"
      ],
      "metadata": {
        "id": "OzTdH-IJVmQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy  = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall    = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1        = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(\"Accuracy :\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall   :\", recall)\n",
        "print(\"F1 Score :\", f1)\n",
        "\n",
        "print(\"\\nDetailed Report:\\n\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai7S-4cUVtCx",
        "outputId": "9bff5a7d-ebc5-42b9-cccb-1ee7273ae42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.9311023622047244\n",
            "Precision: 0.9359118928020793\n",
            "Recall   : 0.9311023622047244\n",
            "F1 Score : 0.9307775728867141\n",
            "\n",
            "Detailed Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       1.00      0.98      0.99        81\n",
            "       glass       0.98      0.84      0.90       101\n",
            "       metal       0.82      0.96      0.89        82\n",
            "       paper       0.96      0.99      0.98       119\n",
            "     plastic       0.90      0.94      0.92        97\n",
            "       trash       0.95      0.75      0.84        28\n",
            "\n",
            "    accuracy                           0.93       508\n",
            "   macro avg       0.94      0.91      0.92       508\n",
            "weighted avg       0.94      0.93      0.93       508\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xticks(np.arange(NUM_CLASSES), class_names, rotation=45)\n",
        "plt.yticks(np.arange(NUM_CLASSES), class_names)\n",
        "\n",
        "for i in range(NUM_CLASSES):\n",
        "    for j in range(NUM_CLASSES):\n",
        "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "gunPosmSWPAg",
        "outputId": "cc364769-3091-4c7b-97ac-7eec6a563d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(class_names, class_acc)\n",
        "plt.title(\"Class-wise Accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "hhdSCExyVvhv",
        "outputId": "5d3812a6-8e40-4d4e-ba91-08ed5b662131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(class_names, class_acc)\n",
        "plt.title(\"Class-wise Accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "vRt0lMnJVxfu",
        "outputId": "ad660c52-ca42-4372-e76b-c767ada2c497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binary_labels = label_binarize(all_labels, classes=range(NUM_CLASSES))\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "\n",
        "for i in range(NUM_CLASSES):\n",
        "    fpr, tpr, _ = roc_curve(binary_labels[:, i], all_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC={roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0,1],[0,1], linestyle='--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve (Multi-class)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "gEjokIkBWs9_",
        "outputId": "c58d03a2-b69d-4f98-c79f-3decd0319db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"evaluation_metrics.txt\",\"w\") as f:\n",
        "    f.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    f.write(f\"Precision: {precision}\\n\")\n",
        "    f.write(f\"Recall: {recall}\\n\")\n",
        "    f.write(f\"F1 Score: {f1}\\n\")\n",
        "\n",
        "print(\"Metrics saved to evaluation_metrics.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZvb26h5Wthh",
        "outputId": "c44a4ddb-5d09-4b6d-9ee1-43b63030c701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to evaluation_metrics.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MobileNetV2 Training Script for Kria KV260 Deployment\n",
        "Updated with modern PyTorch quantization API (PyTorch 2.0+)\n",
        "\n",
        "Implements best practices for edge AI deployment including:\n",
        "- Post-Training Quantization (PTQ) - simpler and more compatible\n",
        "- Model optimization for DPU\n",
        "- ONNX export\n",
        "- Comprehensive evaluation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Training configuration optimized for Kria KV260\"\"\"\n",
        "\n",
        "    # Data settings\n",
        "    DATA_DIR = \"./data\"  # Update this to your dataset path\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 4\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.1\n",
        "    TEST_SPLIT = 0.1\n",
        "\n",
        "    # Model settings - Kria KV260 optimization\n",
        "    INPUT_SIZE = 224  # Standard MobileNetV2 input\n",
        "    NUM_CLASSES = 2  # Binary classification (update for your use case)\n",
        "    PRETRAINED = True\n",
        "\n",
        "    # Training settings\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    EARLY_STOPPING_PATIENCE = 7\n",
        "\n",
        "    # Quantization settings for Kria KV260 DPU\n",
        "    # Using Post-Training Quantization (PTQ) instead of QAT for better compatibility\n",
        "    QUANTIZE = True\n",
        "\n",
        "    # Export settings\n",
        "    EXPORT_ONNX = True\n",
        "    ONNX_OPSET = 11  # Compatible with most deployment tools\n",
        "\n",
        "    # Output settings\n",
        "    OUTPUT_DIR = \"./outputs\"\n",
        "    CHECKPOINT_DIR = \"./checkpoints\"\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "def get_data_transforms():\n",
        "    \"\"\"\n",
        "    Define data augmentation and normalization for MobileNetV2\n",
        "    Uses ImageNet statistics for transfer learning\n",
        "    \"\"\"\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(Config.INPUT_SIZE),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(Config.INPUT_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform\n",
        "\n",
        "def prepare_dataloaders():\n",
        "    \"\"\"Prepare train, validation, and test dataloaders\"\"\"\n",
        "\n",
        "    train_transform, val_test_transform = get_data_transforms()\n",
        "\n",
        "    # Load full dataset\n",
        "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(Config.TRAIN_SPLIT * total_size)\n",
        "    val_size = int(Config.VAL_SPLIT * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Apply transforms\n",
        "    train_dataset.dataset.transform = train_transform\n",
        "    val_dataset.dataset.transform = val_test_transform\n",
        "    test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
        "    print(f\"Class names: {full_dataset.classes}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, full_dataset.classes\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "def create_mobilenetv2_model(num_classes=Config.NUM_CLASSES, pretrained=Config.PRETRAINED):\n",
        "    \"\"\"\n",
        "    Create MobileNetV2 model optimized for Kria KV260\n",
        "\n",
        "    Key optimizations:\n",
        "    - Uses pretrained weights for better convergence\n",
        "    - Modifies classifier for custom number of classes\n",
        "    - Adds dropout for regularization\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        model = models.mobilenet_v2(weights=None)\n",
        "\n",
        "    # Modify the classifier for custom number of classes\n",
        "    # MobileNetV2 has 1280 features before the classifier\n",
        "    in_features = model.classifier[1].in_features\n",
        "\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "# ============================================================================\n",
        "# POST-TRAINING QUANTIZATION (Simpler and more compatible than QAT)\n",
        "# ============================================================================\n",
        "\n",
        "def calibrate_model(model, dataloader, device, num_batches=100):\n",
        "    \"\"\"\n",
        "    Calibrate model for quantization using sample data\n",
        "    This is used for Post-Training Quantization (PTQ)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(tqdm(dataloader, desc=\"Calibrating\")):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            model(inputs)\n",
        "\n",
        "def quantize_model_ptq(model, val_loader, device):\n",
        "    \"\"\"\n",
        "    Apply Post-Training Quantization (PTQ)\n",
        "    More reliable than QAT for deployment to Kria KV260\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß Applying Post-Training Quantization...\")\n",
        "\n",
        "    # Move model to CPU for quantization\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "\n",
        "    # Set quantization configuration\n",
        "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "    # Prepare for quantization (inserts observers)\n",
        "    model_prepared = torch.quantization.prepare(model, inplace=False)\n",
        "\n",
        "    # Calibrate with representative data\n",
        "    calibrate_model(model_prepared, val_loader, torch.device('cpu'), num_batches=50)\n",
        "\n",
        "    # Convert to quantized model\n",
        "    model_quantized = torch.quantization.convert(model_prepared, inplace=False)\n",
        "\n",
        "    print(\"‚úì Post-Training Quantization completed\")\n",
        "\n",
        "    return model_quantized\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Main training function with all best practices\"\"\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"MobileNetV2 Training for Kria KV260 Deployment\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {Config.DEVICE}\")\n",
        "    print(f\"Quantization: {'PTQ (Post-Training)' if Config.QUANTIZE else 'Disabled'}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"\\nPreparing data...\")\n",
        "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
        "\n",
        "    # Create model\n",
        "    print(\"\\nCreating model...\")\n",
        "    model = create_mobilenetv2_model(num_classes=Config.NUM_CLASSES)\n",
        "    model = model.to(Config.DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                          lr=Config.LEARNING_RATE,\n",
        "                          weight_decay=Config.WEIGHT_DECAY)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Starting Training...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{Config.NUM_EPOCHS}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, Config.DEVICE)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, Config.DEVICE)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            early_stopping_counter = 0\n",
        "            checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"‚úì Best model saved (Val Acc: {val_acc:.4f})\")\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n‚ö† Early stopping triggered after {epoch + 1} epochs\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Training Completed!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    print(\"\\nLoading best model...\")\n",
        "    checkpoint = torch.load(os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"‚úì Best model loaded (Val Acc: {checkpoint['val_acc']:.4f})\")\n",
        "\n",
        "    # Apply quantization if enabled\n",
        "    model_quantized = None\n",
        "    if Config.QUANTIZE:\n",
        "        model_quantized = quantize_model_ptq(model, val_loader, Config.DEVICE)\n",
        "\n",
        "    return model, model_quantized, history, class_names, test_loader\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, test_loader, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Evaluating {model_name} on Test Set...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Handle quantized models (output may be different type)\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            else:\n",
        "                # For quantized models, convert to tensor\n",
        "                outputs_tensor = torch.from_numpy(outputs) if isinstance(outputs, np.ndarray) else outputs\n",
        "                probs = torch.softmax(outputs_tensor, dim=1)\n",
        "                _, preds = torch.max(outputs_tensor, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nüìä Test Set Metrics:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        'model': model_name,\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1),\n",
        "        'confusion_matrix': conf_matrix.tolist(),\n",
        "        'class_names': class_names\n",
        "    }\n",
        "\n",
        "    return metrics, conf_matrix\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EXPORT (for Kria KV260)\n",
        "# ============================================================================\n",
        "\n",
        "def export_model(model, model_quantized, class_names):\n",
        "    \"\"\"Export model for Kria KV260 deployment\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Exporting Model for Deployment...\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    model = model.cpu()\n",
        "\n",
        "    # 1. Save PyTorch model (FP32)\n",
        "    pytorch_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_fp32.pth')\n",
        "    torch.save(model.state_dict(), pytorch_path)\n",
        "    print(f\"‚úì FP32 PyTorch model saved to {pytorch_path}\")\n",
        "\n",
        "    # 2. Save quantized model if available\n",
        "    if model_quantized is not None:\n",
        "        quant_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_int8.pth')\n",
        "        torch.save(model_quantized.state_dict(), quant_path)\n",
        "        print(f\"‚úì INT8 Quantized model saved to {quant_path}\")\n",
        "\n",
        "    # 3. Export to ONNX (for Vitis AI deployment on Kria)\n",
        "    if Config.EXPORT_ONNX:\n",
        "        dummy_input = torch.randn(1, 3, Config.INPUT_SIZE, Config.INPUT_SIZE)\n",
        "        onnx_path = os.path.join(Config.OUTPUT_DIR, 'mobilenetv2_model.onnx')\n",
        "\n",
        "        try:\n",
        "            torch.onnx.export(\n",
        "                model,\n",
        "                dummy_input,\n",
        "                onnx_path,\n",
        "                export_params=True,\n",
        "                opset_version=Config.ONNX_OPSET,\n",
        "                do_constant_folding=True,\n",
        "                input_names=['input'],\n",
        "                output_names=['output'],\n",
        "                dynamic_axes={\n",
        "                    'input': {0: 'batch_size'},\n",
        "                    'output': {0: 'batch_size'}\n",
        "                }\n",
        "            )\n",
        "            print(f\"‚úì ONNX model saved to {onnx_path}\")\n",
        "\n",
        "            # Verify ONNX model\n",
        "            try:\n",
        "                import onnx\n",
        "                onnx_model = onnx.load(onnx_path)\n",
        "                onnx.checker.check_model(onnx_model)\n",
        "                print(\"‚úì ONNX model verification passed\")\n",
        "            except ImportError:\n",
        "                print(\"‚ö† ONNX not installed, skipping verification\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† ONNX verification warning: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† ONNX export failed: {e}\")\n",
        "\n",
        "    # 4. Save model metadata\n",
        "    metadata = {\n",
        "        'model_name': 'MobileNetV2',\n",
        "        'input_size': Config.INPUT_SIZE,\n",
        "        'num_classes': Config.NUM_CLASSES,\n",
        "        'class_names': class_names,\n",
        "        'quantized': Config.QUANTIZE,\n",
        "        'quantization_type': 'PTQ' if Config.QUANTIZE else 'None',\n",
        "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'preprocessing': {\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225]\n",
        "        },\n",
        "        'framework': 'PyTorch',\n",
        "        'pytorch_version': torch.__version__\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(Config.OUTPUT_DIR, 'model_metadata.json')\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "    print(f\"‚úì Model metadata saved to {metadata_path}\")\n",
        "\n",
        "    # 5. Create deployment guide\n",
        "    deployment_guide = f\"\"\"\n",
        "# MobileNetV2 Deployment Guide for Kria KV260\n",
        "\n",
        "## Model Information\n",
        "- Architecture: MobileNetV2\n",
        "- Input Size: {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Number of Classes: {Config.NUM_CLASSES}\n",
        "- Classes: {', '.join(class_names)}\n",
        "- Quantization: {'PTQ (INT8)' if Config.QUANTIZE else 'FP32 only'}\n",
        "\n",
        "## Files Generated\n",
        "1. mobilenetv2_fp32.pth - PyTorch FP32 model weights\n",
        "2. mobilenetv2_int8.pth - PyTorch INT8 quantized model (if enabled)\n",
        "3. mobilenetv2_model.onnx - ONNX format for Vitis AI\n",
        "4. model_metadata.json - Model configuration\n",
        "5. test_metrics.json - Performance metrics\n",
        "\n",
        "## Preprocessing Requirements\n",
        "Images must be preprocessed with:\n",
        "- Resize to 256x256\n",
        "- Center crop to {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\n",
        "- Normalize with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "\n",
        "## Deployment Steps for Kria KV260\n",
        "\n",
        "### Method 1: Using Vitis AI (Recommended)\n",
        "\n",
        "#### Step 1: Quantize ONNX Model\n",
        "```bash\n",
        "# On development machine with Vitis AI Docker\n",
        "./docker_run.sh xilinx/vitis-ai-pytorch:latest\n",
        "\n",
        "# Inside container\n",
        "vai_q_pytorch \\\\\n",
        "  --model mobilenetv2_model.onnx \\\\\n",
        "  --quant_mode calib \\\\\n",
        "  --output_dir quantized/\n",
        "```\n",
        "\n",
        "#### Step 2: Compile for DPU\n",
        "```bash\n",
        "vai_c_xir \\\\\n",
        "  --xmodel quantized/mobilenetv2_int.xmodel \\\\\n",
        "  --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json \\\\\n",
        "  --output_dir compiled/\n",
        "```\n",
        "\n",
        "#### Step 3: Deploy on Kria KV260\n",
        "```python\n",
        "import vart\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load DPU runner\n",
        "dpu = vart.Runner.create_runner(\"mobilenetv2.xmodel\", \"run\")\n",
        "\n",
        "# Preprocess image\n",
        "img = Image.open(\"test.jpg\").resize(({Config.INPUT_SIZE}, {Config.INPUT_SIZE}))\n",
        "img = np.array(img).astype(np.float32) / 255.0\n",
        "img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "img = np.expand_dims(img.transpose(2, 0, 1), 0)\n",
        "\n",
        "# Run inference\n",
        "output = dpu.execute_async([img])[0]\n",
        "prediction = np.argmax(output)\n",
        "```\n",
        "\n",
        "### Method 2: Using PyTorch on Kria (No DPU)\n",
        "\n",
        "If deploying without DPU acceleration:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "model = mobilenet_v2(num_classes={Config.NUM_CLASSES})\n",
        "model.load_state_dict(torch.load('mobilenetv2_fp32.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop({Config.INPUT_SIZE}),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Inference\n",
        "img = Image.open(\"test.jpg\")\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    prediction = output.argmax(1).item()\n",
        "```\n",
        "\n",
        "## Expected Performance\n",
        "\n",
        "### FP32 Model\n",
        "- Accuracy: See test_metrics_fp32.json\n",
        "- CPU Inference: ~50-100ms per image\n",
        "- Memory: ~14MB\n",
        "\n",
        "### INT8 Quantized Model (with DPU)\n",
        "- Accuracy: See test_metrics_int8.json (typically 0.5-1% drop from FP32)\n",
        "- DPU Inference: ~15-30ms per image\n",
        "- Throughput: ~30-60 FPS\n",
        "- Memory: ~3.5MB\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### ONNX Export Issues\n",
        "- Ensure PyTorch >= 2.0\n",
        "- Try reducing opset version if needed\n",
        "- Check for custom layers\n",
        "\n",
        "### Quantization Accuracy Drop\n",
        "- Increase calibration batches\n",
        "- Use more representative calibration data\n",
        "- Consider fine-tuning after quantization\n",
        "\n",
        "### DPU Compilation Errors\n",
        "- Verify arch.json matches your Kria version\n",
        "- Check ONNX model compatibility\n",
        "- Ensure all operations are DPU-supported\n",
        "\n",
        "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "\n",
        "    guide_path = os.path.join(Config.OUTPUT_DIR, 'DEPLOYMENT_GUIDE.md')\n",
        "    with open(guide_path, 'w') as f:\n",
        "        f.write(deployment_guide)\n",
        "    print(f\"‚úì Deployment guide saved to {guide_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "    axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = os.path.join(Config.OUTPUT_DIR, 'training_history.png')\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Training history plot saved to {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names, model_name=\"Model\"):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
        "    cm_path = os.path.join(Config.OUTPUT_DIR, filename)\n",
        "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Confusion matrix saved to {cm_path}\")\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Train model\n",
        "        model, model_quantized, history, class_names, test_loader = train_model()\n",
        "\n",
        "        # Plot training history\n",
        "        plot_training_history(history)\n",
        "\n",
        "        # Evaluate FP32 model\n",
        "        metrics_fp32, cm_fp32 = evaluate_model(model.to(Config.DEVICE), test_loader, class_names, \"FP32 Model\")\n",
        "        plot_confusion_matrix(cm_fp32, class_names, \"FP32 Model\")\n",
        "\n",
        "        # Save FP32 metrics\n",
        "        with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_fp32.json'), 'w') as f:\n",
        "            json.dump(metrics_fp32, f, indent=4)\n",
        "\n",
        "        # Evaluate quantized model if available\n",
        "        if model_quantized is not None:\n",
        "            metrics_int8, cm_int8 = evaluate_model(model_quantized, test_loader, class_names, \"INT8 Quantized\")\n",
        "            plot_confusion_matrix(cm_int8, class_names, \"INT8 Quantized\")\n",
        "\n",
        "            # Save INT8 metrics\n",
        "            with open(os.path.join(Config.OUTPUT_DIR, 'test_metrics_int8.json'), 'w') as f:\n",
        "                json.dump(metrics_int8, f, indent=4)\n",
        "\n",
        "            # Compare metrics\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"Model Comparison\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"FP32 Accuracy:  {metrics_fp32['accuracy']:.4f}\")\n",
        "            print(f\"INT8 Accuracy:  {metrics_int8['accuracy']:.4f}\")\n",
        "            print(f\"Accuracy Drop:  {(metrics_fp32['accuracy'] - metrics_int8['accuracy']):.4f}\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        # Export models\n",
        "        export_model(model, model_quantized, class_names)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚úì Pipeline Completed Successfully!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nAll outputs saved to: {Config.OUTPUT_DIR}\")\n",
        "        print(f\"Model checkpoints saved to: {Config.CHECKPOINT_DIR}\")\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Review test_metrics_*.json for model performance\")\n",
        "        print(\"2. Check DEPLOYMENT_GUIDE.md for Kria KV260 deployment\")\n",
        "        print(\"3. Use Vitis AI to compile the ONNX model for DPU\")\n",
        "        if model_quantized:\n",
        "            print(\"4. Compare FP32 vs INT8 performance trade-offs\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"- Check if DATA_DIR path is correct\")\n",
        "        print(\"- Verify dataset has at least 2 classes\")\n",
        "        print(\"- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\")\n",
        "        print(\"- Check all required packages are installed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBYds4_DMs8d",
        "outputId": "9daaf124-6a38-4823-9275-84270bdca410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MobileNetV2 Training for Kria KV260 Deployment\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Quantization: PTQ (Post-Training)\n",
            "======================================================================\n",
            "\n",
            "Preparing data...\n",
            "\n",
            "‚ùå Error occurred: [Errno 2] No such file or directory: './data'\n",
            "\n",
            "Troubleshooting tips:\n",
            "- Check if DATA_DIR path is correct\n",
            "- Verify dataset has at least 2 classes\n",
            "- Ensure sufficient GPU memory (reduce BATCH_SIZE if needed)\n",
            "- Check all required packages are installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 763, in main\n",
            "    model, model_quantized, history, class_names, test_loader = train_model()\n",
            "                                                                ^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 320, in train_model\n",
            "    train_loader, val_loader, test_loader, class_names = prepare_dataloaders()\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2860976906.py\", line 106, in prepare_dataloaders\n",
            "    full_dataset = datasets.ImageFolder(root=Config.DATA_DIR)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 328, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 149, in __init__\n",
            "    classes, class_to_idx = self.find_classes(self.root)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 234, in find_classes\n",
            "    return find_classes(directory)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\", line 41, in find_classes\n",
            "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
            "                                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Install PyTorch and onnxscript if needed\n",
        "!pip install torch --quiet\n",
        "!pip install onnxscript --quiet\n",
        "\n",
        "# Step 1: Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "from google.colab import files\n",
        "\n",
        "# Step 2: Upload your .pth file\n",
        "print(\"üì§ Upload your .pth file\")\n",
        "uploaded = files.upload()  # Choose your .pth file\n",
        "\n",
        "# Get uploaded filename\n",
        "pth_filename = list(uploaded.keys())[0]\n",
        "print(\"Uploaded file:\", pth_filename)\n",
        "\n",
        "# Step 3: Define your model architecture (MobileNetV2)\n",
        "# Use the same architecture definition as during training\n",
        "def create_mobilenetv2_model(num_classes, pretrained=True):\n",
        "    if pretrained:\n",
        "        model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        model = models.mobilenet_v2(weights=None)\n",
        "\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Step 4: Load checkpoint\n",
        "checkpoint = torch.load(pth_filename, map_location=\"cpu\")\n",
        "\n",
        "# Extract necessary information from the checkpoint\n",
        "# Based on previous cells, the checkpoint contains 'model_state_dict', 'class_names', 'input_size'\n",
        "num_classes = len(checkpoint['class_names']) if 'class_names' in checkpoint else 6 # Default to 6 if not found\n",
        "input_size = checkpoint['input_size'] if 'input_size' in checkpoint else 224 # Default to 224 if not found\n",
        "\n",
        "model = create_mobilenetv2_model(num_classes=num_classes, pretrained=False) # No need for pretrained weights when loading state_dict\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "# Step 5: Create dummy input (match training input size)\n",
        "dummy_input = torch.randn(1, 3, input_size, input_size) # MobileNetV2 expects 3 channels, e.g., 224x224\n",
        "\n",
        "# Step 6: Export to ONNX\n",
        "onnx_path = \"model.onnx\"\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        ")\n",
        "print(f\"‚úÖ ONNX model exported to {onnx_path}\")\n",
        "\n",
        "# Step 7: Download ONNX model\n",
        "files.download(onnx_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "MKqrTvrmTGDW",
        "outputId": "f72ad964-c5e1-4c37-86c4-00861a14adb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m689.1/689.1 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.3/159.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hüì§ Upload your .pth file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a27c466-f712-4dbe-934d-1cc6d00df5d8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5a27c466-f712-4dbe-934d-1cc6d00df5d8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving best_model.pth to best_model (2).pth\n",
            "Uploaded file: best_model (2).pth\n",
            "‚úÖ Model loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-547310851.py:53: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
            "  torch.onnx.export(\n",
            "W0211 10:48:48.637000 3177 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 11 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
            "W0211 10:48:49.465000 3177 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0211 10:48:49.466000 3177 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0211 10:48:49.468000 3177 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "W0211 10:48:49.469000 3177 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `MobileNetV2([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `MobileNetV2([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ‚úÖ\n",
            "[torch.onnx] Translate the graph into ONNX...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:onnxscript.version_converter:The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 11).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:onnxscript.version_converter:Failed to convert the model to the target version 11 using the ONNX C API. The model was not modified\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 120, in call\n",
            "    converted_proto = _c_api_utils.call_onnx_api(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
            "    result = func(proto)\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 115, in _partial_convert_version\n",
            "    return onnx.version_converter.convert_version(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx/version_converter.py\", line 39, in convert_version\n",
            "    converted_model_str = C.convert_version(model_str, target_version)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: /github/workspace/onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied 104 of general pattern rewrite rules.\n",
            "‚úÖ ONNX model exported to model.onnx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_03c3e7e8-9f21-4104-b018-3b70b1fbb64c\", \"model.onnx\", 250076)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}